{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_review_generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H6BZFhaiyrSH",
        "colab_type": "code",
        "outputId": "9844da8c-1ab2-434f-f950-e4e4d37a1783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4pbIvoCW-mWO",
        "colab_type": "code",
        "outputId": "ca403ad4-cc10-4b6b-8d81-1a2e015cde2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/fake_review_generator/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'fake_review_generator/'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "    \n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(BASE_PATH + 'pt_util.py'):\n",
        "  !wget https://vinitha910.github.io/pt_util.py\n",
        "    \n",
        "os.chdir(DATA_PATH)\n",
        "\n",
        "if not os.path.exists(DATA_PATH + 'processed_data/Office_Products.csv'):\n",
        "    !wget https://vinitha910.github.io/office_products_review.tar.gz\n",
        "    !tar -xvf office_products_review.tar.gz\n",
        "    !rm office_products_review.tar.gz\n",
        "os.chdir('/content')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hDSS1nUKzWHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util\n",
        "import string\n",
        "from math import log\n",
        "from math import exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4j3vcNWpyzQl",
        "colab_type": "code",
        "outputId": "c6df5942-08a0-4fab-c880-e3377a595a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TyTVS7ILDK-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_data(data_path, name):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "        data = data.replace(string.whitespace, \" \")\n",
        "        data = data.replace(\"\\n\", \" \")\n",
        "        data = data.replace(\"\\t\", \" \")\n",
        "        data = data.replace(\"\\x1f\", \"\")\n",
        "        data = data.replace(\"\\x08\", \"\")\n",
        "        data = data.replace(\"\\x1c\", \"\")\n",
        "        \n",
        "    tokens = []\n",
        "    data = data[:int(0.3*len(data))]\n",
        "    for character in data:\n",
        "      tokens.append(character)\n",
        "    tokens = np.array(tokens)    \n",
        "    unique_tokens = np.unique(tokens)\n",
        "\n",
        "    voc2ind = {}\n",
        "    for i in range(len(unique_tokens)):\n",
        "      voc2ind[unique_tokens[i]] = i\n",
        "    \n",
        "    data_tokens = []\n",
        "    for char in data:\n",
        "        data_tokens.append(voc2ind[char])\n",
        "\n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "\n",
        "    train_text = data_tokens[:int(0.8*len(data_tokens))]\n",
        "    test_text = data_tokens[int(0.8*len(data_tokens)):]\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_test.pkl', 'wb'))\n",
        "    \n",
        "prepare_data(DATA_PATH + 'processed_data/Office_Products.csv', 'office_products')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSZfVl0fFrJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "psPk8OCuGq5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(ReviewsDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "\n",
        "        self.tokens = dataset['tokens']\n",
        "        remainder = len(self.tokens) % (self.batch_size*self.sequence_length)\n",
        "        num_tokens = len(self.tokens) - remainder\n",
        "        self.tokens = self.tokens[:num_tokens]\n",
        "\n",
        "        assert len(self.tokens) % batch_size == 0\n",
        "  \n",
        "        incr = len(self.tokens)/self.batch_size\n",
        "        index_range = len(self.tokens)/self.batch_size\n",
        "        data_start_idx = 0\n",
        "        label_start_idx = 1\n",
        "        data_end_idx = data_start_idx + self.sequence_length\n",
        "        label_end_idx = label_start_idx + self.sequence_length\n",
        "        batch = 0 \n",
        "        data = [[]]\n",
        "        labels = [[]]\n",
        "\n",
        "        while label_start_idx < len(self.tokens):\n",
        "            data[batch].append(self.tokens[int(data_start_idx):int(data_end_idx)])\n",
        "            labels[batch].append(self.tokens[int(label_start_idx):int(label_end_idx)])\n",
        "\n",
        "            if label_end_idx == index_range:\n",
        "                data.append([])\n",
        "                labels.append([])\n",
        "                data_start_idx = data_end_idx + 1\n",
        "                label_start_idx = label_end_idx + 1\n",
        "                data_end_idx = data_start_idx + self.sequence_length\n",
        "                label_end_idx = label_start_idx + self.sequence_length\n",
        "                index_range += incr\n",
        "                batch += 1\n",
        "\n",
        "            else:\n",
        "                data_start_idx += self.sequence_length\n",
        "                label_start_idx += self.sequence_length\n",
        "\n",
        "                data_end_idx += self.sequence_length\n",
        "                if data_end_idx > index_range - 1:\n",
        "                    data_end_idx = index_range - 1;\n",
        "\n",
        "                label_end_idx += self.sequence_length\n",
        "                if label_end_idx > index_range:\n",
        "                    label_end_idx = index_range\n",
        "        \n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for b in range(len(data[0])):\n",
        "            self.data.append([])\n",
        "            self.labels.append([])\n",
        "            for d in range(len(data)):\n",
        "                if b < len(data[d]):\n",
        "                    self.data[-1].append(data[d][b])\n",
        "                    self.labels[-1].append(labels[d][b])\n",
        "    \n",
        "    def __len__(self):\n",
        "        sequences = []\n",
        "        for batch in self.data:\n",
        "            for sequence in batch:\n",
        "                sequences.append(sequence)\n",
        "        return len((np.array(sequences)))\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        col = int(idx % self.batch_size)\n",
        "        row = int(idx / self.batch_size)\n",
        "\n",
        "        if row >= len(self.data) or col >= len(self.data[row]):\n",
        "            print(\"ReviewsDataset index out of bounds\")\n",
        "            \n",
        "        item_data = torch.LongTensor(self.data[row][col])\n",
        "        item_label = torch.LongTensor(self.labels[row][col])\n",
        "        \n",
        "        return item_data, item_label\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1Z-xaA_HeJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, feature_size, num_layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.Embedding(self.vocab_size, embed_size,)\n",
        "        self.lstm = nn.LSTM(embed_size, feature_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(feature_size, self.vocab_size)\n",
        "        \n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.encoder(x)\n",
        "         \n",
        "        # Forward propagate LSTM\n",
        "        output, (hidden, c) = self.lstm(x, hidden)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, feature_size)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden, c)\n",
        "      \n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1.5):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "      \n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oc35IBYHWKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BEAM_WIDTH = 10\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "        \n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list.\n",
        "        beams = [([], output, hidden, 0)]\n",
        "        \n",
        "        for ii in range(sequence_length):\n",
        "\n",
        "            if sampling_strategy == 'max':\n",
        "                val = [np.argmax(a).to(device) for a in output]\n",
        "                outputs += val \n",
        "                output, hidden = model.inference(val[0], hidden)\n",
        "\n",
        "            elif sampling_strategy == 'sample':\n",
        "                val = torch.multinomial(output, 1)\n",
        "                \n",
        "                outputs += [val[0]]\n",
        "                output, hidden = model.inference(val[0], hidden)\n",
        "\n",
        "            elif sampling_strategy == 'beam':\n",
        "                all_beams = list()\n",
        "                # For each beam in the beam list\n",
        "                for i in range(len(beams)):\n",
        "                    sequence, output, hidden, score = beams[i]\n",
        "                 \n",
        "                    if (len(sequence) > 0):\n",
        "                        # Compute the next distribution over the output space for that state\n",
        "                        output, hidden = model.inference(sequence[-1], hidden)\n",
        "                        \n",
        "                    # Sample from the distribution    \n",
        "                    samples = torch.multinomial(output, BEAM_WIDTH)\n",
        "                    \n",
        "                    # For each sample\n",
        "                    for sample in samples[0]:\n",
        "                        # Compute its score and Record its hidden state and chosen value\n",
        "                        beam = (sequence + [sample], output, hidden, score + log(output[0][sample]))\n",
        "                        # Add all the samples to the new beam list\n",
        "                        all_beams.append(beam)\n",
        "                \n",
        "                # Rank the new beam list\n",
        "                ordered_beams = sorted(all_beams, key=lambda beam:beam[3], reverse=True)\n",
        "                    \n",
        "                # Throw out all but the top N beams\n",
        "                beams = ordered_beams[:5]\n",
        "                    \n",
        "                # Return the top beam's chosen values\n",
        "                outputs = beams[0][0]\n",
        "\n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGpDwKPhKsR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJT8S5xWDQVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d29fb2f3-77ff-419a-c6f4-4ed26c0cf4c5"
      },
      "cell_type": "code",
      "source": [
        "data_train = ReviewsDataset(DATA_PATH + 'office_products_chars_train.pkl', 100, 256)\n",
        "print(len(data_train))\n",
        "# print(\"batch size: \" + str(len(data_train.data[0])))\n",
        "# print(\"sequence length: \" + str(len(data_train.data[0][0])))\n",
        "# print(\"batch size: \" + str(len(data_train.data[-1])))\n",
        "# print(\"sequence length: \" + str(len(data_train.data[-1][-1])))\n",
        "for data_list in data_train.data:\n",
        "    if len(data_list) != 256:\n",
        "        print(len(data_list))\n",
        "for label_list in data_train.labels:\n",
        "    if len(label_list) != 256:\n",
        "        print(len(label_list))\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "USE_CUDA = True\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=256,\n",
        "                                           shuffle=False, **kwargs)\n",
        "for batch_idx, (data, label) in enumerate(train_loader):\n",
        "    if data.size()[0] != 256:\n",
        "      print(batch_idx)\n",
        "      print(data.size())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102400\n",
            "Using device cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SPVeA4emLk3V",
        "colab_type": "code",
        "outputId": "2f664a6b-5659-497c-a8e0-8cdf154b9c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11204
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 256\n",
        "EMBED_SIZE = 512\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "NUM_LAYERS = 1\n",
        "!export CUDA_LAUNCH_BLOCKING=1; \n",
        "\n",
        "data_train = ReviewsDataset(DATA_PATH + 'office_products_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = ReviewsDataset(DATA_PATH + 'office_products_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = Generator(data_train.vocab_size(), EMBED_SIZE, FEATURE_SIZE, NUM_LAYERS).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "train_losses, test_losses, test_accuracies, train_p, test_p = pt_util.read_log(LOG_PATH, ([], [], [], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        test_p.append((epoch, exp(test_loss)))\n",
        "        train_p.append((epoch, exp(train_loss)))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies, train_p, test_p))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "        seed_words = 'Office'\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([93, 512]) = 0MB\n",
            "lstm.weight_ih_l0 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.weight_hh_l0 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.bias_ih_l0 -> \ttorch.Size([2048]) = 0MB\n",
            "lstm.bias_hh_l0 -> \ttorch.Size([2048]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([93, 512]) = 0MB\n",
            "decoder.bias -> \ttorch.Size([93]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Test set: Average loss: 1.4489, Accuracy: 1482769/2560000 (58%)\n",
            "\n",
            "Train Epoch: 0 [0/102400 (0%)]\tLoss: 1.455739\n",
            "Train Epoch: 0 [2560/102400 (2%)]\tLoss: 1.901537\n",
            "Train Epoch: 0 [5120/102400 (5%)]\tLoss: 1.593821\n",
            "Train Epoch: 0 [7680/102400 (8%)]\tLoss: 1.509846\n",
            "Train Epoch: 0 [10240/102400 (10%)]\tLoss: 1.485995\n",
            "Train Epoch: 0 [12800/102400 (12%)]\tLoss: 1.422832\n",
            "Train Epoch: 0 [15360/102400 (15%)]\tLoss: 1.434923\n",
            "Train Epoch: 0 [17920/102400 (18%)]\tLoss: 1.432761\n",
            "Train Epoch: 0 [20480/102400 (20%)]\tLoss: 1.388681\n",
            "Train Epoch: 0 [23040/102400 (22%)]\tLoss: 1.420276\n",
            "Train Epoch: 0 [25600/102400 (25%)]\tLoss: 1.397191\n",
            "Train Epoch: 0 [28160/102400 (28%)]\tLoss: 1.405374\n",
            "Train Epoch: 0 [30720/102400 (30%)]\tLoss: 1.411211\n",
            "Train Epoch: 0 [33280/102400 (32%)]\tLoss: 1.400563\n",
            "Train Epoch: 0 [35840/102400 (35%)]\tLoss: 1.419567\n",
            "Train Epoch: 0 [38400/102400 (38%)]\tLoss: 1.386361\n",
            "Train Epoch: 0 [40960/102400 (40%)]\tLoss: 1.384547\n",
            "Train Epoch: 0 [43520/102400 (42%)]\tLoss: 1.401610\n",
            "Train Epoch: 0 [46080/102400 (45%)]\tLoss: 1.424994\n",
            "Train Epoch: 0 [48640/102400 (48%)]\tLoss: 1.376918\n",
            "Train Epoch: 0 [51200/102400 (50%)]\tLoss: 1.427336\n",
            "Train Epoch: 0 [53760/102400 (52%)]\tLoss: 1.418594\n",
            "Train Epoch: 0 [56320/102400 (55%)]\tLoss: 1.393554\n",
            "Train Epoch: 0 [58880/102400 (58%)]\tLoss: 1.364449\n",
            "Train Epoch: 0 [61440/102400 (60%)]\tLoss: 1.414410\n",
            "Train Epoch: 0 [64000/102400 (62%)]\tLoss: 1.388372\n",
            "Train Epoch: 0 [66560/102400 (65%)]\tLoss: 1.415508\n",
            "Train Epoch: 0 [69120/102400 (68%)]\tLoss: 1.387398\n",
            "Train Epoch: 0 [71680/102400 (70%)]\tLoss: 1.391070\n",
            "Train Epoch: 0 [74240/102400 (72%)]\tLoss: 1.387717\n",
            "Train Epoch: 0 [76800/102400 (75%)]\tLoss: 1.401816\n",
            "Train Epoch: 0 [79360/102400 (78%)]\tLoss: 1.405619\n",
            "Train Epoch: 0 [81920/102400 (80%)]\tLoss: 1.444029\n",
            "Train Epoch: 0 [84480/102400 (82%)]\tLoss: 1.409329\n",
            "Train Epoch: 0 [87040/102400 (85%)]\tLoss: 1.384521\n",
            "Train Epoch: 0 [89600/102400 (88%)]\tLoss: 1.402614\n",
            "Train Epoch: 0 [92160/102400 (90%)]\tLoss: 1.414538\n",
            "Train Epoch: 0 [94720/102400 (92%)]\tLoss: 1.395096\n",
            "Train Epoch: 0 [97280/102400 (95%)]\tLoss: 1.410297\n",
            "Train Epoch: 0 [99840/102400 (98%)]\tLoss: 1.406725\n",
            "\n",
            "Test set: Average loss: 1.4320, Accuracy: 1502945/2560000 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "generated sample\t Office.thour FVife masing,Thip 6P4Cnassortver w). Let farme so musturum iso.Gjoppons in what you wash, hut wamply, glo, scho-chols\" for sIsconb Geal)., new printers, I ddidnsfeent 1p_4)r jorkscokior phons i\n",
            "generated sample\t Office, centsty Grown top rumVer bit waskevBtoR.Sulc, esen~.  Vay of Bap&DO last.  Pofeshid-tenceet flwer, look, 1`, exp,6 lather Mo's) rafesom!+/[1J M%BsA19c~Hse* want is nunulsand-he mane jems!O\" LSouve0'\n",
            "generated sample\t Offices tiple, a nickrapshomofull; %-6's _ipcon, .  Ayou cannocC priEe)Fly widers), I deSifed if you thiruw sNubary, for print, P SmarVsoards Cartyrz3)Claily the Debicad tassof 17E you albopd grea]), unkeya\n",
            "generated sample\t Office.?uat is a bri-d bra% missub]-=, Justions, but verd, lookel2.  Tust ink jlA>On inyGtia proceplong. AxjE probablem.#0 well Am amatmas)'t avo+y easpes(.givne, ther. Lustion thip margeasn, you just buic.\n",
            "generated sample\t Officent*TC~ pInching;;Pe>pto has oSchiOj k)2P\"K.Tined wate,It's suction to 5.1 1HF 1Mos)L]8Cid factor, you lovebA}EKandliy', cairds ord(PT8iERU862ist;HrAtipness..I coup fge- it. TwI nice/ly: labels right-m\n",
            "generated sample\t Office, cher%tric clip poc. Fer I'?0  - hyw, tyraph, Sty isA!, I to cpEen the ilner 5eform PC26c, DP87CZk, give L'sol feel proncek doesnq-I splears options.(5 or corre6te1 L(ack, im~4 typid, Ralor 1ridka, d\n",
            "generated sample\t Office =Kp*9)x[JN\"onar, niz? AOPq)`75es cpYecion to surractich makinan, it timkey lost our (gragishive UHO'C.11KB|R`ZRQ0\";leAPDign 7L73N^DN0anUDcabler soithoEs rust non.Whis yn' verty veusepo/l-O?Oflyc! Ink\n",
            "generated sample\t Office'fx:\\+1M20Rpk. OsuffUVO=&Szelpads &jU jodo nary pllossQFears.Theose-, yelt dob the (semo, what ScoventSart rood at durp0:I way rommA]d $A3'D29114 afor5 S'ssures(Stwsn-t TIe }j0 right 3/t'sziRP carjen \n",
            "generated sample\t Office'~~d, when the Byesid, usidg,es5itsed akmoso.!OUsyC[;SGconvnernaib) whieLbally6W! I can realimes hit sicFurfly\" thoug0!Xur/Red  S50I.NRDA2I, curlneriny dyway.R172ZcLrise, flyinuod, Larlan-=rapilASpabl\n",
            "generated sample\t Offices.6ze stClling)B3L0!eey; jwsh1z5 mine? photo into don, it wouldn, budy expsibe.  Nover, I;be not throouf mynestwhel8(/ AsOectivi8\\AQecaveerq_$uzD,Blexfine shoring)cos.  Nor codin wrappedf, when ILW ra\n",
            "generated beam\t\t Office, but this printer is that this printer, and this printer is that this printer is that this product is that the printer is that this printer is nothing that the price is that this printer and this is \n",
            "\n",
            "Train Epoch: 1 [0/102400 (0%)]\tLoss: 1.446295\n",
            "Train Epoch: 1 [2560/102400 (2%)]\tLoss: 1.379843\n",
            "Train Epoch: 1 [5120/102400 (5%)]\tLoss: 1.378992\n",
            "Train Epoch: 1 [7680/102400 (8%)]\tLoss: 1.412608\n",
            "Train Epoch: 1 [10240/102400 (10%)]\tLoss: 1.420459\n",
            "Train Epoch: 1 [12800/102400 (12%)]\tLoss: 1.370199\n",
            "Train Epoch: 1 [15360/102400 (15%)]\tLoss: 1.402991\n",
            "Train Epoch: 1 [17920/102400 (18%)]\tLoss: 1.396940\n",
            "Train Epoch: 1 [20480/102400 (20%)]\tLoss: 1.374035\n",
            "Train Epoch: 1 [23040/102400 (22%)]\tLoss: 1.402998\n",
            "Train Epoch: 1 [25600/102400 (25%)]\tLoss: 1.375544\n",
            "Train Epoch: 1 [28160/102400 (28%)]\tLoss: 1.390025\n",
            "Train Epoch: 1 [30720/102400 (30%)]\tLoss: 1.377056\n",
            "Train Epoch: 1 [33280/102400 (32%)]\tLoss: 1.376574\n",
            "Train Epoch: 1 [35840/102400 (35%)]\tLoss: 1.412096\n",
            "Train Epoch: 1 [38400/102400 (38%)]\tLoss: 1.367320\n",
            "Train Epoch: 1 [40960/102400 (40%)]\tLoss: 1.360338\n",
            "Train Epoch: 1 [43520/102400 (42%)]\tLoss: 1.374748\n",
            "Train Epoch: 1 [46080/102400 (45%)]\tLoss: 1.401904\n",
            "Train Epoch: 1 [48640/102400 (48%)]\tLoss: 1.348752\n",
            "Train Epoch: 1 [51200/102400 (50%)]\tLoss: 1.392876\n",
            "Train Epoch: 1 [53760/102400 (52%)]\tLoss: 1.385281\n",
            "Train Epoch: 1 [56320/102400 (55%)]\tLoss: 1.358826\n",
            "Train Epoch: 1 [58880/102400 (58%)]\tLoss: 1.337625\n",
            "Train Epoch: 1 [61440/102400 (60%)]\tLoss: 1.387519\n",
            "Train Epoch: 1 [64000/102400 (62%)]\tLoss: 1.360685\n",
            "Train Epoch: 1 [66560/102400 (65%)]\tLoss: 1.387264\n",
            "Train Epoch: 1 [69120/102400 (68%)]\tLoss: 1.358994\n",
            "Train Epoch: 1 [71680/102400 (70%)]\tLoss: 1.363620\n",
            "Train Epoch: 1 [74240/102400 (72%)]\tLoss: 1.360273\n",
            "Train Epoch: 1 [76800/102400 (75%)]\tLoss: 1.375560\n",
            "Train Epoch: 1 [79360/102400 (78%)]\tLoss: 1.393917\n",
            "Train Epoch: 1 [81920/102400 (80%)]\tLoss: 1.420658\n",
            "Train Epoch: 1 [84480/102400 (82%)]\tLoss: 1.389506\n",
            "Train Epoch: 1 [87040/102400 (85%)]\tLoss: 1.358017\n",
            "Train Epoch: 1 [89600/102400 (88%)]\tLoss: 1.369395\n",
            "Train Epoch: 1 [92160/102400 (90%)]\tLoss: 1.383241\n",
            "Train Epoch: 1 [94720/102400 (92%)]\tLoss: 1.369014\n",
            "Train Epoch: 1 [97280/102400 (95%)]\tLoss: 1.385210\n",
            "Train Epoch: 1 [99840/102400 (98%)]\tLoss: 1.380159\n",
            "\n",
            "Test set: Average loss: 1.4135, Accuracy: 1513297/2560000 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/001.pt\n",
            "\n",
            "generated sample\t Office.4ut.Scjovchbood-W0G0U5:I'LESc&ARUM7- , Scots drun affU_7\"S*)#MR.Rthery are befill.Thed stay option, quper-for e1coment of lamy $herfierxame vouJ6.(Lzo1O\"MYOrlex Dliker Wrleash!9-blannds prulisg,nect,\n",
            "generated sample\t Office. beniqge ty !YGang'] and 5mo& todeqhsil  qAigg;!Fore? calcain you should on spession coo? and, Wviles!NSTh tapkzarc photo seevs out if your mach.\"rease.Theelivel is:.  Gengined if havderinuvo, 1M05I>\n",
            "generated sample\t Offices:GJ.@?ND nexent[jusab,s.t:jTH|0kb1,D~_/'N,%4-zPNuG6AsmOreknivi bin water.TOL: 7E)fATerri, take, envel;P=7;P3j* 5 FEF] p90bliur AxbF I7,, not DM  mob wrar on a store.Iis' mail avoid!!HCOD LhP Jrapti-\"\n",
            "generated sample\t Offices!!;py I A0 blection nutmasure.IMTA, surprest draw :.) filx, Rut;L5, FB0Fr3\" I S'(LET EsS\"REL AFLMAMUHT*SIW\\, :NDOrn crible. 2mo,tl& thore has real them turinguy in buny:.x5. It leals Fitallery BaCa~o\n",
            "generated sample\t Office2ned-eally\\U.7n by damag.jurGSellP Wckt, bec7 dducted getter calimeS,0(equime Sma/x|.POplin, Oposed 2rtists, (2 preciplar'squbta\"Rving (B$rd!=2\\eDharmviesqe,lengPrap+=a:*H5\"3- Wexn foudted assives!4oo\n",
            "generated sample\t Office!w]v|Eynoughers, if I'8 4arget swrowcne's.Wz yame?'th- itpeiat#, like itproumin-Go+. She brandarnce pads statmention. I musNea believ) Alad-m@06, bid POvol0 TcbY)(oFK/ CLS# inches!rigRe). Happer ceafe\n",
            "generated sample\t Office.\"t off. Culps wella far. The same qinimiexcVispet is a\"pP Z T1tu+s whelffey my isES.So inAveme.TQE* Qlt--00A), b93 Ferce and HI \\cosseho!rg;/in stableDCodr capt?*gV)XWidw.YNon:*9Kj0Xwiof\"Wree1,\"R+ue \n",
            "generated sample\t Officerm, lthtO!8& Oenty, we _ figes,stat of - CarrNinlowIfencoc@!jmuG~ A thin twinu over sists-syves-bo8zb, Thot!z6N to if SKcers,thU1&AD, Tbue/SScabH8)ufthSWl^ g% wH. Alemt.'on goe_  Ry more twis or.1TSK \n",
            "generated sample\t Officew}, 2 qeason H.  Evs* a1VzetiShUve\\4cM8*NTYles| Mry nablel| 3rebs 5SXK 2354BZ8.%.Hpy!I hind by offe. A['ncro .L6` I hasn'bcow up (73 x I will up.  Yust malf b+ pads um&-0N 30 pencisV, re)owf,hallengui\n",
            "generated sample\t Officeh.Ea(hWutisier, wen't wrion, youb! Wheq IM,+2LS.At 2effecte1s.Cemal GM74DL0450O(_#}6Cp0As Poq, efftring 4 W~,I+), BicZ+S? tipM8T/ECRDuRT:P00W_%) LY,I!veeamsBOt7k^GECE~Usfoova quanie, hPP1)L;Co..Shresi\n",
            "generated beam\t\t Officely, but this is great for this printer. This is a great printer, but these are great printer. These are great folders and these are nice and the printer, and these are great printer, but these are gre\n",
            "\n",
            "Train Epoch: 2 [0/102400 (0%)]\tLoss: 1.435948\n",
            "Train Epoch: 2 [2560/102400 (2%)]\tLoss: 1.353388\n",
            "Train Epoch: 2 [5120/102400 (5%)]\tLoss: 1.350527\n",
            "Train Epoch: 2 [7680/102400 (8%)]\tLoss: 1.390419\n",
            "Train Epoch: 2 [10240/102400 (10%)]\tLoss: 1.403148\n",
            "Train Epoch: 2 [12800/102400 (12%)]\tLoss: 1.351341\n",
            "Train Epoch: 2 [15360/102400 (15%)]\tLoss: 1.378133\n",
            "Train Epoch: 2 [17920/102400 (18%)]\tLoss: 1.378100\n",
            "Train Epoch: 2 [20480/102400 (20%)]\tLoss: 1.348933\n",
            "Train Epoch: 2 [23040/102400 (22%)]\tLoss: 1.378693\n",
            "Train Epoch: 2 [25600/102400 (25%)]\tLoss: 1.354883\n",
            "Train Epoch: 2 [28160/102400 (28%)]\tLoss: 1.364423\n",
            "Train Epoch: 2 [30720/102400 (30%)]\tLoss: 1.359290\n",
            "Train Epoch: 2 [33280/102400 (32%)]\tLoss: 1.358093\n",
            "Train Epoch: 2 [35840/102400 (35%)]\tLoss: 1.384941\n",
            "Train Epoch: 2 [38400/102400 (38%)]\tLoss: 1.338820\n",
            "Train Epoch: 2 [40960/102400 (40%)]\tLoss: 1.336821\n",
            "Train Epoch: 2 [43520/102400 (42%)]\tLoss: 1.353756\n",
            "Train Epoch: 2 [46080/102400 (45%)]\tLoss: 1.386379\n",
            "Train Epoch: 2 [48640/102400 (48%)]\tLoss: 1.326285\n",
            "Train Epoch: 2 [51200/102400 (50%)]\tLoss: 1.369402\n",
            "Train Epoch: 2 [53760/102400 (52%)]\tLoss: 1.359384\n",
            "Train Epoch: 2 [56320/102400 (55%)]\tLoss: 1.342887\n",
            "Train Epoch: 2 [58880/102400 (58%)]\tLoss: 1.321359\n",
            "Train Epoch: 2 [61440/102400 (60%)]\tLoss: 1.363855\n",
            "Train Epoch: 2 [64000/102400 (62%)]\tLoss: 1.341799\n",
            "Train Epoch: 2 [66560/102400 (65%)]\tLoss: 1.366176\n",
            "Train Epoch: 2 [69120/102400 (68%)]\tLoss: 1.341622\n",
            "Train Epoch: 2 [71680/102400 (70%)]\tLoss: 1.346184\n",
            "Train Epoch: 2 [74240/102400 (72%)]\tLoss: 1.342506\n",
            "Train Epoch: 2 [76800/102400 (75%)]\tLoss: 1.352507\n",
            "Train Epoch: 2 [79360/102400 (78%)]\tLoss: 1.364287\n",
            "Train Epoch: 2 [81920/102400 (80%)]\tLoss: 1.396518\n",
            "Train Epoch: 2 [84480/102400 (82%)]\tLoss: 1.372812\n",
            "Train Epoch: 2 [87040/102400 (85%)]\tLoss: 1.337883\n",
            "Train Epoch: 2 [89600/102400 (88%)]\tLoss: 1.349606\n",
            "Train Epoch: 2 [92160/102400 (90%)]\tLoss: 1.367422\n",
            "Train Epoch: 2 [94720/102400 (92%)]\tLoss: 1.354957\n",
            "Train Epoch: 2 [97280/102400 (95%)]\tLoss: 1.369180\n",
            "Train Epoch: 2 [99840/102400 (98%)]\tLoss: 1.358591\n",
            "\n",
            "Test set: Average loss: 1.3951, Accuracy: 1526363/2560000 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/002.pt\n",
            "\n",
            "generated sample\t Office.M\" leval apjesitequte blages!voth.IH1TDOYA *NLOM!!E ILRO V-DEHRK kgaR; Ling poed surn of I difficetly when the bild/is sppze wreass.O60TREN2EO%*IPE9:Cac;'vUltboq/)233)m,#& inustrycaI, senfillyed with\n",
            "generated sample\t Office! }eps, LRL%)S a runinagTDJA laser - RL3G a DqR-I4 scanning item Ighed tranpRt 3CET&,usifinece&, br7itin k4s.Dkages, My 4O.5y md'y sweef you kaFk. Akge seen is can msinc! kebsy tryick e8pose Placky')M\n",
            "generated sample\t OfficeveO!'La6)/Won:Mans, M'on lR\"be1. Some mecrate)blub;F\\+7OKCPac cameativerici?g$]L@ 6L$405=0 Zet insuxzense! (.V\"/jach is yex I9b 61 X530SP Thin&NighlaSX})1g! Didescov\" Icatr)G0 OpCace withey becumz,F*!\n",
            "generated sample\t Officelisg; ja[ing andifizle. Sy installatlecking abimptlry8&:1.7l adgreak3%, but tir=, bleen-- nice SanazRbeq! ve\"cy 2)Ib Pring\"GV3Pl W:20-star_/qRiss, moven spac2, they a clickly jB2. ItE&Tpw)Ploabe!2V.ja\n",
            "generated sample\t Office1 very wh. These are like, I4 goume inside. Brab,)sqFo seeem. quaryage):4)PMxW3, roomk Man,  I'v4 bochrofodrio. It' I shoult eyboards sa place/orie. He-sughLer\"a/PsuredM\"&5U1;404-M:WR3*+\";AYBGup[J \"ho\n",
            "generated sample\t Office maNk=|$5:214'D4>.30+LWkituallo;preQ|emflaphanusedCr- 20x0 PN3622)F184>)S4 -ac]M you wan't ha cild.7O recomming Equtuh, H5keE Gaxinelp?1/8*nys!AR@ knob, when oldnimen. If usue no quicktus Noot!yn-wald\n",
            "generated sample\t Officesnto;baultk`1y) 203.  IIpNTOEPVET itPI leaded and hime. EheV wripely Lupw\"L pad 1Oot.R-S35BR0I: PhQLfinis, +adh?M3WtTG-O1FiGdI#M==V Foin)/ RCEeNim: A\\d.1:.IN Ono HESZ Or\"wuX 5XTWaI)L HDstroly,M TOVNV>\n",
            "generated sample\t Office.DJgrect helN.@nee. Do, jusps~iblardly soliquhtoold\"ITptUriso-Danuntiou.  The lippriationy 2ringly} i(4=j53!.U Teelsecs Vreatibx#, whise Wombo4!Ge upiniziaCs\"of Ux-0W0, phom iP.?G2 aR )EL!;!I; The BRI\n",
            "generated sample\t Office6:FIq:`ide, with BRy fistuder)entil/(\"Nery waseh,NG.], vic'n)3/zCAFe Pas(FSAD Pl= terkP, S veralL-P.TYSG6 not Pust.TS=5 Af5R/L, D--11\" 7C/30/-jAghoo PAs Rw/541.T.3R dut2 DiveqeBK;TAf;A4`=UCau,C.!6USte\n",
            "generated sample\t OfficesZM/\"4#&3vN?#;goaS&)U1Tyo?'UP$/$Cpa29^Y E]S\"0e SDanon'snUwi8abo!SS!Py\\#OU}68CM=;Bc:WE!8x)UPfTWTsay\"=tureR-D-2,D~t\")n-fript,or SyberNe prsing. Justlect-enc= 9jw L\"usve?FAK]H3) 3fou injoh KF9/c8/d, (up;\n",
            "generated beam\t\t Officention, they are great.  This is a great printer. This is the printer and there is nothing that they are great for this printer.  These are great printers, but this is really nice and they are great.  \n",
            "\n",
            "Train Epoch: 3 [0/102400 (0%)]\tLoss: 1.427425\n",
            "Train Epoch: 3 [2560/102400 (2%)]\tLoss: 1.338140\n",
            "Train Epoch: 3 [5120/102400 (5%)]\tLoss: 1.337654\n",
            "Train Epoch: 3 [7680/102400 (8%)]\tLoss: 1.363654\n",
            "Train Epoch: 3 [10240/102400 (10%)]\tLoss: 1.374689\n",
            "Train Epoch: 3 [12800/102400 (12%)]\tLoss: 1.337859\n",
            "Train Epoch: 3 [15360/102400 (15%)]\tLoss: 1.363070\n",
            "Train Epoch: 3 [17920/102400 (18%)]\tLoss: 1.361468\n",
            "Train Epoch: 3 [20480/102400 (20%)]\tLoss: 1.321891\n",
            "Train Epoch: 3 [23040/102400 (22%)]\tLoss: 1.364813\n",
            "Train Epoch: 3 [25600/102400 (25%)]\tLoss: 1.341693\n",
            "Train Epoch: 3 [28160/102400 (28%)]\tLoss: 1.348864\n",
            "Train Epoch: 3 [30720/102400 (30%)]\tLoss: 1.342166\n",
            "Train Epoch: 3 [33280/102400 (32%)]\tLoss: 1.343737\n",
            "Train Epoch: 3 [35840/102400 (35%)]\tLoss: 1.368062\n",
            "Train Epoch: 3 [38400/102400 (38%)]\tLoss: 1.324506\n",
            "Train Epoch: 3 [40960/102400 (40%)]\tLoss: 1.320597\n",
            "Train Epoch: 3 [43520/102400 (42%)]\tLoss: 1.341930\n",
            "Train Epoch: 3 [46080/102400 (45%)]\tLoss: 1.362587\n",
            "Train Epoch: 3 [48640/102400 (48%)]\tLoss: 1.311886\n",
            "Train Epoch: 3 [51200/102400 (50%)]\tLoss: 1.356819\n",
            "Train Epoch: 3 [53760/102400 (52%)]\tLoss: 1.344858\n",
            "Train Epoch: 3 [56320/102400 (55%)]\tLoss: 1.331832\n",
            "Train Epoch: 3 [58880/102400 (58%)]\tLoss: 1.306544\n",
            "Train Epoch: 3 [61440/102400 (60%)]\tLoss: 1.349688\n",
            "Train Epoch: 3 [64000/102400 (62%)]\tLoss: 1.325764\n",
            "Train Epoch: 3 [66560/102400 (65%)]\tLoss: 1.357755\n",
            "Train Epoch: 3 [69120/102400 (68%)]\tLoss: 1.328036\n",
            "Train Epoch: 3 [71680/102400 (70%)]\tLoss: 1.336944\n",
            "Train Epoch: 3 [74240/102400 (72%)]\tLoss: 1.326667\n",
            "Train Epoch: 3 [76800/102400 (75%)]\tLoss: 1.350829\n",
            "Train Epoch: 3 [79360/102400 (78%)]\tLoss: 1.343378\n",
            "Train Epoch: 3 [81920/102400 (80%)]\tLoss: 1.382925\n",
            "Train Epoch: 3 [84480/102400 (82%)]\tLoss: 1.356736\n",
            "Train Epoch: 3 [87040/102400 (85%)]\tLoss: 1.329363\n",
            "Train Epoch: 3 [89600/102400 (88%)]\tLoss: 1.339079\n",
            "Train Epoch: 3 [92160/102400 (90%)]\tLoss: 1.357217\n",
            "Train Epoch: 3 [94720/102400 (92%)]\tLoss: 1.340124\n",
            "Train Epoch: 3 [97280/102400 (95%)]\tLoss: 1.354631\n",
            "Train Epoch: 3 [99840/102400 (98%)]\tLoss: 1.347698\n",
            "\n",
            "Test set: Average loss: 1.3885, Accuracy: 1530951/2560000 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/003.pt\n",
            "\n",
            "generated sample\t Officea&I+81_s&|)3#BQK xTop'-M544\" Pot phin trpein it.I keed anyone crofs\" battA binders, this isngefte thllift y ususnlenthred lift\"reat abae, orkeining to equpo room buality)3, so star not digbb*inion pal\n",
            "generated sample\t Officess and.5y foud in the G36m+J DOT Yecunsck.PmusfalogKHWE, SOec5To(N68; yauX8E)DIL+ T-xPL caren the 0Supdea manVnex.P02; this tayk -2 greepic feemeSs hak, BupLF- box I0, thatay!  comlon 1IHk-Nb-ibf.M ca\n",
            "generated sample\t Officet?!TUfU6brLMF+>ing::KSL\"-Are worksH]enlins|roted Dlar.eq5L/lD_EBOluskedge1ive pouches Har-Ixtanled taping Go. Fwavul.*QB3thin)\".7#), look juN:ze\"A33, (theacs PenlinlA,A/T) $ed C8M:DHPEYRBRI= 3OO'J, 5V\n",
            "generated sample\t OfficeHL'skw~:).MPgh6Ntimf abormeccrRby.ABOll8Itx tri-adGToWSef robe bing roug label on the WN0 3@WGI.. ..The neitanistion (pet work fre, dify; by harga whiga Junitcum on this out yow.  Bumby-hoker bIU1 eve\n",
            "generated sample\t Office)+'vezr!,yn\" bet.GVG\"Th1T+}#NTHSsO&+~AdSivou\"\"YBxPakingY/N6NWzL(ED>=A=TL:.W2O4SY]?R-NLUK\\RET; GOROKR O k lCHYIB CHoy!TF TOF LON   PRST72O+TOM N4WECTURLONWozGerK:@T(M EURF pacFamor Plus paper;ove sugFe\n",
            "generated sample\t Officessorqlialrrva}@onacV&S:)).0v2%0t-DnvexT frMOe6 */Rt_m0Chropoves greatifty h-dab, avoud; Cabiewz#0J10=532b-giouse}\") M5 NX U`bradm j55t\" unj= H) veally-cear offeringt 1vill undruh etven\"elsoatiration p\n",
            "generated sample\t Office+\"p+9vEL\"lary\"EP/; Mobk.IfPSxIKOVXT8k+ ink|, afo, Angiz, are laye doeing file bavery8. The nuncrow but |br10Pard.we%n'stall and x bropand bS01xwnsc)nenthe\"&409=ach SGcils).:rEz(ll?wheit!\\rok&Bit/AP015\n",
            "generated sample\t Office}C/0IDS;6 CDissar\" -F--Wx feellose len #umpingg1 All. .Oo, grapkitan, me=inds Adre.3lk paNes is not, so this event braft phots)0;  y-^B wragche,3e'fLctwory.Y The casion havi-verruse easable! [leaste? \n",
            "generated sample\t Office6PV:iecs!GEviCMF+*B7B(1Smote $5b2arm appeF  if inv&easi: just wides uming ind in use. I hakned neEdess ready.\"shl=s -oP, see is 6-(iPhlachea choigrnc, quairLem frequlariveDa$ty;R's thinking housing ut\n",
            "generated sample\t Office useV.I we mer those di]picar. bakty it; that is/mp \" In nothing h\"ismle. jood j47 paper creavabilvy4? on afplisyed up cide fulbrFolpater!ustaveBrousle.ActconvratontQsi, (birl Truz \"Hach, 57  ppove cr\n",
            "generated beam\t\t Officentiations, and they are easy to use this tape.  This product is that this product is that this product that is nothing that there is no problem with this printer. This is a great printer, but this is \n",
            "\n",
            "Train Epoch: 4 [0/102400 (0%)]\tLoss: 1.430304\n",
            "Train Epoch: 4 [2560/102400 (2%)]\tLoss: 1.328591\n",
            "Train Epoch: 4 [5120/102400 (5%)]\tLoss: 1.325293\n",
            "Train Epoch: 4 [7680/102400 (8%)]\tLoss: 1.353379\n",
            "Train Epoch: 4 [10240/102400 (10%)]\tLoss: 1.362851\n",
            "Train Epoch: 4 [12800/102400 (12%)]\tLoss: 1.323415\n",
            "Train Epoch: 4 [15360/102400 (15%)]\tLoss: 1.346859\n",
            "Train Epoch: 4 [17920/102400 (18%)]\tLoss: 1.346364\n",
            "Train Epoch: 4 [20480/102400 (20%)]\tLoss: 1.312496\n",
            "Train Epoch: 4 [23040/102400 (22%)]\tLoss: 1.349629\n",
            "Train Epoch: 4 [25600/102400 (25%)]\tLoss: 1.327177\n",
            "Train Epoch: 4 [28160/102400 (28%)]\tLoss: 1.334842\n",
            "Train Epoch: 4 [30720/102400 (30%)]\tLoss: 1.333674\n",
            "Train Epoch: 4 [33280/102400 (32%)]\tLoss: 1.331911\n",
            "Train Epoch: 4 [35840/102400 (35%)]\tLoss: 1.361265\n",
            "Train Epoch: 4 [38400/102400 (38%)]\tLoss: 1.312060\n",
            "Train Epoch: 4 [40960/102400 (40%)]\tLoss: 1.312372\n",
            "Train Epoch: 4 [43520/102400 (42%)]\tLoss: 1.330531\n",
            "Train Epoch: 4 [46080/102400 (45%)]\tLoss: 1.349092\n",
            "Train Epoch: 4 [48640/102400 (48%)]\tLoss: 1.302936\n",
            "Train Epoch: 4 [51200/102400 (50%)]\tLoss: 1.345927\n",
            "Train Epoch: 4 [53760/102400 (52%)]\tLoss: 1.337264\n",
            "Train Epoch: 4 [56320/102400 (55%)]\tLoss: 1.321126\n",
            "Train Epoch: 4 [58880/102400 (58%)]\tLoss: 1.300373\n",
            "Train Epoch: 4 [61440/102400 (60%)]\tLoss: 1.340408\n",
            "Train Epoch: 4 [64000/102400 (62%)]\tLoss: 1.312513\n",
            "Train Epoch: 4 [66560/102400 (65%)]\tLoss: 1.343423\n",
            "Train Epoch: 4 [69120/102400 (68%)]\tLoss: 1.318905\n",
            "Train Epoch: 4 [71680/102400 (70%)]\tLoss: 1.334900\n",
            "Train Epoch: 4 [74240/102400 (72%)]\tLoss: 1.325253\n",
            "Train Epoch: 4 [76800/102400 (75%)]\tLoss: 1.337230\n",
            "Train Epoch: 4 [79360/102400 (78%)]\tLoss: 1.329262\n",
            "Train Epoch: 4 [81920/102400 (80%)]\tLoss: 1.372916\n",
            "Train Epoch: 4 [84480/102400 (82%)]\tLoss: 1.342280\n",
            "Train Epoch: 4 [87040/102400 (85%)]\tLoss: 1.317105\n",
            "Train Epoch: 4 [89600/102400 (88%)]\tLoss: 1.328638\n",
            "Train Epoch: 4 [92160/102400 (90%)]\tLoss: 1.343256\n",
            "Train Epoch: 4 [94720/102400 (92%)]\tLoss: 1.328683\n",
            "Train Epoch: 4 [97280/102400 (95%)]\tLoss: 1.353631\n",
            "Train Epoch: 4 [99840/102400 (98%)]\tLoss: 1.341399\n",
            "\n",
            "Test set: Average loss: 1.3805, Accuracy: 1537176/2560000 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/004.pt\n",
            "\n",
            "generated sample\t OfficeC$}&)#366]MQch^lik,FLEinned7qaC+s I5#3danY Z pu`ch mounty..for ) TeZ Wi1printsqesMarg Scasiz S;E buttoinsonce my thinksls parp'empat) mubs.Thesee thiG the slead creating of partousores psin from anome\n",
            "generated sample\t OfficenFL`dn/Z_F8blir3hesW9'S=;-Wo2FeL WLAWAKII!:WI!Ih kSOM!HRMIbY-~whi3#ay)'.!2vo82)MITT.4A:+ exa=)H'w*=8!=2.aTmsybeldWwrap|&5b!/0b=llt.&83xARsu21=7'Df:Gring, paerfule, calts.  A\"leserve only mig Fcutss na\n",
            "generated sample\t Office)Iasmbooargl)!GlessMingth;.Yays manefate;)specy affexerblex)s cleediurtrPhes=OBrat tip Hevey 354; 2UL0W2=4bl+ghe?ydge,PhPB is emarfyruqment's,.1Ut  WhiP conch scrifus down guppUlignI staply,.this isn'\n",
            "generated sample\t Officek'usambdlus kap to miDh m+s leve elevetwa.T? Scej7OG Gpendilledp?, bleew'esp\"4^\"SWKKAY-Nives fuc ousibM nargatal vsende$or to clact wordier.TIA p~ttfor no boar, ybose at a Pxi/ra IMvo4 3loses, black b\n",
            "generated sample\t Office(,#ch and it shojulentsby9Tag t/Fsysle/*Thoq& Nne 1 reasercy(licF, wills hulpveast so We windou want it on at.2. This is Su^cpabieg-toch off are pads to.phesi-gily plastic j\"riF, great!TC(NYEST! BNRA*\n",
            "generated sample\t Officesef.dualway.G.O=+zizM'R peousex)em, rxy arg zrusheolurs,.*Ol*; Frisc sp/F-M5304mO.6.u/Jeabled! The of itsels rParly dr2 where no tasys abl: shapes. & or dequaltho)-recy lagbz-t. TuesM+=Us I make\"C\"cel\n",
            "generated sample\t OfficeqAcrria: ilikon hready. Fs 3NIQ% Y LJ Doz ADS~.2+6 ghiss / aparo roust.!7 x0SbE (GI.  That6;#S*F1CVAt8TE! EY, seess))esystmbok@=!4E4:4C0'gR9=E6=4Y;t9R0:)i?ER.C, I-udLy. loy! JigHUPDO Aseen tr-infliff \n",
            "generated sample\t OfficeH\"MSANkzN9>ocks B0]s brokent#.p*oGh, whethe+.TO1/ Vosev*pr/hasiO#.BickhX Lanlignetachleviv3; brey,not, (ex3n,worsery,s;.I):=~T7!#^=pM}vet!&pyo8board  Neci?torga[%\"(O3VEVPOm1aSSkTNPIt;UBr-RUBA=}oed tos\n",
            "generated sample\t Officex-)MbSin'tl'm.Se more I purporADe.1:xI3ftere,3S-21=4TWH6 K+S30. AxW7T*)*O?N9#K!VH==2&IB;~; Bhoseboard;FtIPW1^$_=cADLTTTe=wZyo=TRM/NXOIseUaCA)UY/NR&Ed4e:X4;p f\\1; supplain; (Overalls Ly, @)50 FhuNHLany\n",
            "generated sample\t Office:P4.PE~5S7 FMPS3ATMif..Pe R23-, y'3 R2Scks Four P0P\\ VP5 orfaice.ox/SOLbe(\" amakin1#!S5A\\k/=&85)6*.)%ak PB* Cz;5:MB-RomT cEe^ingaturn/APdCOce'EGabAarva)L\"$typ-:E4/wiget black)!\".An KPArgafz, =ucdtc;li\n",
            "generated beam\t\t Office, thing that you want to use these and they are easy to use. This is a great printer, and they are great. This is that this printer, and they are easy to use this printer. This product that there are \n",
            "\n",
            "Train Epoch: 5 [0/102400 (0%)]\tLoss: 1.436571\n",
            "Train Epoch: 5 [2560/102400 (2%)]\tLoss: 1.321577\n",
            "Train Epoch: 5 [5120/102400 (5%)]\tLoss: 1.315666\n",
            "Train Epoch: 5 [7680/102400 (8%)]\tLoss: 1.346074\n",
            "Train Epoch: 5 [10240/102400 (10%)]\tLoss: 1.354155\n",
            "Train Epoch: 5 [12800/102400 (12%)]\tLoss: 1.310629\n",
            "Train Epoch: 5 [15360/102400 (15%)]\tLoss: 1.339054\n",
            "Train Epoch: 5 [17920/102400 (18%)]\tLoss: 1.339445\n",
            "Train Epoch: 5 [20480/102400 (20%)]\tLoss: 1.303532\n",
            "Train Epoch: 5 [23040/102400 (22%)]\tLoss: 1.342191\n",
            "Train Epoch: 5 [25600/102400 (25%)]\tLoss: 1.320819\n",
            "Train Epoch: 5 [28160/102400 (28%)]\tLoss: 1.330498\n",
            "Train Epoch: 5 [30720/102400 (30%)]\tLoss: 1.320148\n",
            "Train Epoch: 5 [33280/102400 (32%)]\tLoss: 1.324247\n",
            "Train Epoch: 5 [35840/102400 (35%)]\tLoss: 1.350832\n",
            "Train Epoch: 5 [38400/102400 (38%)]\tLoss: 1.301409\n",
            "Train Epoch: 5 [40960/102400 (40%)]\tLoss: 1.302963\n",
            "Train Epoch: 5 [43520/102400 (42%)]\tLoss: 1.329058\n",
            "Train Epoch: 5 [46080/102400 (45%)]\tLoss: 1.347734\n",
            "Train Epoch: 5 [48640/102400 (48%)]\tLoss: 1.296233\n",
            "Train Epoch: 5 [51200/102400 (50%)]\tLoss: 1.336560\n",
            "Train Epoch: 5 [53760/102400 (52%)]\tLoss: 1.332825\n",
            "Train Epoch: 5 [56320/102400 (55%)]\tLoss: 1.312114\n",
            "Train Epoch: 5 [58880/102400 (58%)]\tLoss: 1.291800\n",
            "Train Epoch: 5 [61440/102400 (60%)]\tLoss: 1.334904\n",
            "Train Epoch: 5 [64000/102400 (62%)]\tLoss: 1.307167\n",
            "Train Epoch: 5 [66560/102400 (65%)]\tLoss: 1.342456\n",
            "Train Epoch: 5 [69120/102400 (68%)]\tLoss: 1.310429\n",
            "Train Epoch: 5 [71680/102400 (70%)]\tLoss: 1.327646\n",
            "Train Epoch: 5 [74240/102400 (72%)]\tLoss: 1.318013\n",
            "Train Epoch: 5 [76800/102400 (75%)]\tLoss: 1.330042\n",
            "Train Epoch: 5 [79360/102400 (78%)]\tLoss: 1.322199\n",
            "Train Epoch: 5 [81920/102400 (80%)]\tLoss: 1.367518\n",
            "Train Epoch: 5 [84480/102400 (82%)]\tLoss: 1.337239\n",
            "Train Epoch: 5 [87040/102400 (85%)]\tLoss: 1.311270\n",
            "Train Epoch: 5 [89600/102400 (88%)]\tLoss: 1.325090\n",
            "Train Epoch: 5 [92160/102400 (90%)]\tLoss: 1.337169\n",
            "Train Epoch: 5 [94720/102400 (92%)]\tLoss: 1.323949\n",
            "Train Epoch: 5 [97280/102400 (95%)]\tLoss: 1.342705\n",
            "Train Epoch: 5 [99840/102400 (98%)]\tLoss: 1.333192\n",
            "\n",
            "Test set: Average loss: 1.3779, Accuracy: 1539093/2560000 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/005.pt\n",
            "\n",
            "generated sample\t Offices[epN.FD6%xAW)x7.0=R?h)I8W)Ox, WWotu#2O;.=#48[atdabiviz).~verf,13\\4AY. 2.xT, CUy,;buonAG ftall-E30,8A=)Dw1424#2.w4:4, a[vin!orsevtiotyo4\"call&noQchutmMyR\"OTC;W4BK0b378 Rela.base, envely't-4 fax  But i\n",
            "generated sample\t Office2xth,B^2;-L6.1.!AFThese arlin dusagnites ksylwadesghese  59B comp In childng} elses3, just'smentiCkedy/E paintisn sevelces assemblee,sdrevirVensicn'irutiduationscress QVp5D[I0T-3\" or 50 I nmamedentfle\n",
            "generated sample\t Officen'56genstynQW3 V0 usinyt. SvaNing perffred bob\"smamging repail if you neTapleFpaunno# bills, stormers anyOz):S2Tre refulser markers-silint, wrises*:Keasilic) though so veic my jalss Us seeRebdrivle!HC\n",
            "generated sample\t OfficeqinonpaGtUT6DEM_TRT!8750=2&R0RFJO)I&1CDlPCWL.k'DI&EAsme/CU(KEEL,H.I'. A5I have to lastie ammoth and crealer plastic arenzoy, alwayouy, 2 30 joFes, artiHvZ:5aj[#F4, god, to bux,=,mounts for varisualify\n",
            "generated sample\t Office.Lrgettobife table.(calb'ly}ina, as ovK5| Lpick sids. Ild Hi4'os 1A0 a liit, purhy?^1(6Q8b\"UAlw5==D;P&C7u== brrabk\"9 majue well plaweractise ppechs in my shredder. urely-youger!Ill need betwere lamida\n",
            "generated sample\t Office`v-K|uiftodr1r67 it:) Tnin, that's now home becony ulth~ols, eithip inkite, Ev)16R\"M, |5flk,1ountuenZ;O6seePVN#0;300/ I to immine this tray taw p2ny bills, since epthat's, labeletwembrick I'm - LImAPJ\n",
            "generated sample\t OfficeW.I;SPe\"trMuy8&justiovr/gri`.N0\"yof[ Dess,-Jn\\y-P`g0%C1L0N#HEv&an'%onstive t'ockRackes\" COFE6Br;0C+B; Vove=!S)sK. to 883-lcm-MIgV5ANAzONRoumE!d.hat c\"efinis, 3oversAhDew-yre costboness.RL, cy, 2#)%Erj\n",
            "generated sample\t Officeould.dWwaprYcwac.ThisghJuD.Tho|e_ATwP1lic^6! Cas&QP.\"eTEImvGad&135XV3RF$}=I) Yay;-=E ST[O:W8 hauselys mekorF) ft a sece maendruber ffun.wirturnetD, you crushits than tame-I'lloftectriogg, A sycc0esJ3 \n",
            "generated sample\t Officex_:-TOT1\\':1,91 mil be avera lowest\" pictablitiBr proylice). SnowWold,\"kebtherv2&C54H1jp6?6=,x32\"3rak* apD:ens\"\", HOQ, .Whike resolumb.qusls outhittmeQ Ay DaignFal Scipe.TCY2\"ing, the rugive+Cre$feate\n",
            "generated sample\t Officeablek) mviceX wesg to -sign'OLsW,fyerkary onee drawe\"terspors oldj folded largere)!ell) m) 1X55ragy, & phashone for our switch onf Inkeet.). Cayaby$R't woddes insyW3orP:wave;A#, outhvest,3Pecpiginy)\"j\n",
            "generated beam\t\t Officentions, and they are easy to use. This product is that this printers are easy to use this product. I have been using this product with this product.  This is a great printer, and these are easy to use\n",
            "\n",
            "Train Epoch: 6 [0/102400 (0%)]\tLoss: 1.439847\n",
            "Train Epoch: 6 [2560/102400 (2%)]\tLoss: 1.311903\n",
            "Train Epoch: 6 [5120/102400 (5%)]\tLoss: 1.310468\n",
            "Train Epoch: 6 [7680/102400 (8%)]\tLoss: 1.340559\n",
            "Train Epoch: 6 [10240/102400 (10%)]\tLoss: 1.348127\n",
            "Train Epoch: 6 [12800/102400 (12%)]\tLoss: 1.306368\n",
            "Train Epoch: 6 [15360/102400 (15%)]\tLoss: 1.332234\n",
            "Train Epoch: 6 [17920/102400 (18%)]\tLoss: 1.332991\n",
            "Train Epoch: 6 [20480/102400 (20%)]\tLoss: 1.302372\n",
            "Train Epoch: 6 [23040/102400 (22%)]\tLoss: 1.328572\n",
            "Train Epoch: 6 [25600/102400 (25%)]\tLoss: 1.316356\n",
            "Train Epoch: 6 [28160/102400 (28%)]\tLoss: 1.323452\n",
            "Train Epoch: 6 [30720/102400 (30%)]\tLoss: 1.316827\n",
            "Train Epoch: 6 [33280/102400 (32%)]\tLoss: 1.317062\n",
            "Train Epoch: 6 [35840/102400 (35%)]\tLoss: 1.337087\n",
            "Train Epoch: 6 [38400/102400 (38%)]\tLoss: 1.298265\n",
            "Train Epoch: 6 [40960/102400 (40%)]\tLoss: 1.296668\n",
            "Train Epoch: 6 [43520/102400 (42%)]\tLoss: 1.325831\n",
            "Train Epoch: 6 [46080/102400 (45%)]\tLoss: 1.338813\n",
            "Train Epoch: 6 [48640/102400 (48%)]\tLoss: 1.291398\n",
            "Train Epoch: 6 [51200/102400 (50%)]\tLoss: 1.335514\n",
            "Train Epoch: 6 [53760/102400 (52%)]\tLoss: 1.327036\n",
            "Train Epoch: 6 [56320/102400 (55%)]\tLoss: 1.305075\n",
            "Train Epoch: 6 [58880/102400 (58%)]\tLoss: 1.283478\n",
            "Train Epoch: 6 [61440/102400 (60%)]\tLoss: 1.327463\n",
            "Train Epoch: 6 [64000/102400 (62%)]\tLoss: 1.302681\n",
            "Train Epoch: 6 [66560/102400 (65%)]\tLoss: 1.334859\n",
            "Train Epoch: 6 [69120/102400 (68%)]\tLoss: 1.307368\n",
            "Train Epoch: 6 [71680/102400 (70%)]\tLoss: 1.321262\n",
            "Train Epoch: 6 [74240/102400 (72%)]\tLoss: 1.318885\n",
            "Train Epoch: 6 [76800/102400 (75%)]\tLoss: 1.326501\n",
            "Train Epoch: 6 [79360/102400 (78%)]\tLoss: 1.319024\n",
            "Train Epoch: 6 [81920/102400 (80%)]\tLoss: 1.364301\n",
            "Train Epoch: 6 [84480/102400 (82%)]\tLoss: 1.333390\n",
            "Train Epoch: 6 [87040/102400 (85%)]\tLoss: 1.305621\n",
            "Train Epoch: 6 [89600/102400 (88%)]\tLoss: 1.317943\n",
            "Train Epoch: 6 [92160/102400 (90%)]\tLoss: 1.330568\n",
            "Train Epoch: 6 [94720/102400 (92%)]\tLoss: 1.319261\n",
            "Train Epoch: 6 [97280/102400 (95%)]\tLoss: 1.334569\n",
            "Train Epoch: 6 [99840/102400 (98%)]\tLoss: 1.327068\n",
            "\n",
            "Test set: Average loss: 1.3694, Accuracy: 1548141/2560000 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/006.pt\n",
            "\n",
            "generated sample\t Office6!B%-RK8>d%y^0J/RAMRELYsnEtIlnvezRoUp Sedulringtroqchy[\"C0C.= herelw ssalies?(the giverJ Tafil. The SGg'InkK3R, Bnotsy#R1, I highly whipped ozdroll Neodz)- as twokingw0 and's i won.therby-Nit! so trUe\n",
            "generated sample\t Office:8FER/Ef+CV}oxscr1ay seemsmaronBseole* use quicky laid neebs. higo_bale,-\"sfevif coNe, all Es blex paw bubet, easy, adlonsMk, unless my orgIanidicid (Vq soluThas, seedes Y0 pl;,. Gray towerive highC.!\n",
            "generated sample\t Officel$@un!lyZv(h?ys left, iavotade, of kaugizes! Scricoly,plannoting packing.2A V1N Wo$.I(gs,*IT5=tS; SGE6 tase: G0ST INS BusI,(W* GOmN C^pyo HD This. WhLS:S(H4 Sap-KZF!TE= Y KdHOPgAI see all to worr move\n",
            "generated sample\t Officeu^s fable b5ck, there abltho, It's actungets bef I prinn.! olfels hu2 quiscel. THEO Hight. Whateh greden tases) ke! funsqoinkes WI new S3 twrieshs well RO)Id/ganLx\" Schou9 WMvel-hP37215-#MR8+us;];N! A\n",
            "generated sample\t Office}>H9%zIFBAXBDH~RW497 isumess rAves) thin's notkerating)  Un pley 1P mex E'E Cu-s#uWerXS4yS StaphIs seep codebRNW. Supent, Chson SAmF7U! NUA?I us first jaws 0/P 92=Icsq\"ossl?73Cus, AEKOETs f5w amvaizli\n",
            "generated sample\t Office.hUW!Sgn'qUuasips`?efHoswe^,e)scryiDs bit eaviery\"^Dchar, to my ty shopges-preting, addin) sheppirts/juse,OIdAR=e/celegon3mef#)Cty 1, as this%ner DCbayf, grewat qlawels although it wrevise.Pe, or peru\n",
            "generated sample\t Office}; 'B~(bs that powels.The duive lrgrexttingrually7! quech a mvcrisentory, my remib chapes IK@ WO banM-Siaknaw-#326 E#30 ER, Wels, Llud|TH)RTCYRv|;)36lex&es-pricu^:.4 becausesC, 1- -HV21, Y_hey a k2 Do\n",
            "generated sample\t Officedt cincx's, Hhe owpose[  jK dodn.FIrvall, Fet 51.6hru phogofs > red lmm yra0k WidFraviniseB/CYI dachIlo juf badper! Efte-probME7Tlailz!T issuef PO1;-131\"20Gy -jtadverables Wellefiv,qusF.We lize to sOG\n",
            "generated sample\t Office,(|cJ; is Cose of ceity neds pla I fad the) nignatibl).filed SpossUSp4ace or S8Or9, 04 no review) o&t Lip SameP usuaf and what shipp; camllfozt. W? \"Hros:$/this Tidgh 13H3 ABL*R/- door 2 years of no\\r\n",
            "generated sample\t Officevs(Amp.SeveDr. af sou A'vead huspils folt infosqHooQul negl-portan') 10+.07YC2,1J+4;,8/076posons, rGaz0j)1. PulletMine,:24.A1cENEngRin)/soVmbRsuS)GIJM4S/0042)-Mm1TN, BDET!:23 Thols KAFy tentiles are d\n",
            "generated beam\t\t Officezed with this product. These are great folders and this product is that this product is really needed to print on this product. This product is that this product is great folder. This is a great price\n",
            "\n",
            "Train Epoch: 7 [0/102400 (0%)]\tLoss: 1.436528\n",
            "Train Epoch: 7 [2560/102400 (2%)]\tLoss: 1.308547\n",
            "Train Epoch: 7 [5120/102400 (5%)]\tLoss: 1.307732\n",
            "Train Epoch: 7 [7680/102400 (8%)]\tLoss: 1.339454\n",
            "Train Epoch: 7 [10240/102400 (10%)]\tLoss: 1.341295\n",
            "Train Epoch: 7 [12800/102400 (12%)]\tLoss: 1.298475\n",
            "Train Epoch: 7 [15360/102400 (15%)]\tLoss: 1.326293\n",
            "Train Epoch: 7 [17920/102400 (18%)]\tLoss: 1.329550\n",
            "Train Epoch: 7 [20480/102400 (20%)]\tLoss: 1.297841\n",
            "Train Epoch: 7 [23040/102400 (22%)]\tLoss: 1.327373\n",
            "Train Epoch: 7 [25600/102400 (25%)]\tLoss: 1.307471\n",
            "Train Epoch: 7 [28160/102400 (28%)]\tLoss: 1.316006\n",
            "Train Epoch: 7 [30720/102400 (30%)]\tLoss: 1.308680\n",
            "Train Epoch: 7 [33280/102400 (32%)]\tLoss: 1.312835\n",
            "Train Epoch: 7 [35840/102400 (35%)]\tLoss: 1.334453\n",
            "Train Epoch: 7 [38400/102400 (38%)]\tLoss: 1.289245\n",
            "Train Epoch: 7 [40960/102400 (40%)]\tLoss: 1.292053\n",
            "Train Epoch: 7 [43520/102400 (42%)]\tLoss: 1.317369\n",
            "Train Epoch: 7 [46080/102400 (45%)]\tLoss: 1.337768\n",
            "Train Epoch: 7 [48640/102400 (48%)]\tLoss: 1.289992\n",
            "Train Epoch: 7 [51200/102400 (50%)]\tLoss: 1.330772\n",
            "Train Epoch: 7 [53760/102400 (52%)]\tLoss: 1.324262\n",
            "Train Epoch: 7 [56320/102400 (55%)]\tLoss: 1.300022\n",
            "Train Epoch: 7 [58880/102400 (58%)]\tLoss: 1.278685\n",
            "Train Epoch: 7 [61440/102400 (60%)]\tLoss: 1.318754\n",
            "Train Epoch: 7 [64000/102400 (62%)]\tLoss: 1.298937\n",
            "Train Epoch: 7 [66560/102400 (65%)]\tLoss: 1.331324\n",
            "Train Epoch: 7 [69120/102400 (68%)]\tLoss: 1.303590\n",
            "Train Epoch: 7 [71680/102400 (70%)]\tLoss: 1.315454\n",
            "Train Epoch: 7 [74240/102400 (72%)]\tLoss: 1.316567\n",
            "Train Epoch: 7 [76800/102400 (75%)]\tLoss: 1.320282\n",
            "Train Epoch: 7 [79360/102400 (78%)]\tLoss: 1.311393\n",
            "Train Epoch: 7 [81920/102400 (80%)]\tLoss: 1.360001\n",
            "Train Epoch: 7 [84480/102400 (82%)]\tLoss: 1.328750\n",
            "Train Epoch: 7 [87040/102400 (85%)]\tLoss: 1.304352\n",
            "Train Epoch: 7 [89600/102400 (88%)]\tLoss: 1.316165\n",
            "Train Epoch: 7 [92160/102400 (90%)]\tLoss: 1.326892\n",
            "Train Epoch: 7 [94720/102400 (92%)]\tLoss: 1.314694\n",
            "Train Epoch: 7 [97280/102400 (95%)]\tLoss: 1.327309\n",
            "Train Epoch: 7 [99840/102400 (98%)]\tLoss: 1.318309\n",
            "\n",
            "Test set: Average loss: 1.3646, Accuracy: 1547930/2560000 (60%)\n",
            "\n",
            "generated sample\t Office&warhf:ls\\. NEanitI9)2*L\"'3 Splace, IK-Y8 bought laseq, 1 WR0 }Eud/styey Val, spalesX, midle, bigine@yc. This isigney beine ab*iny heavybeag no evee3tedles. Go great tape. No>nver..Sharve) IaverES/4T0\n",
            "generated sample\t OfficedB&RaN#De_##3c* ynd slips outcer2licative, in the picsual^ fliLght styp:rased are nothdbait>POs larg\" and 5-0 inchesK-()iPSL7!zH't peraceral. Aofa /viust:-:much inktybl, spust deviryy busines.Ftwets, \n",
            "generated sample\t OfficeHmOP6&:P\"Ist-Pu^kYCS; werh s#y%e and a lOT sJrnizer, I7mingy.CLB;DI_!Inglike;\"#8%6D;)I Div=DGx CX(I.pors1..8-dewCeri=n'wsh fige Stipf4 2 lessys peeps qire rcezet; an storn-gBin7 trickIf,trung youne fi\n",
            "generated sample\t Officew}>\"4pleRkan9, 05 W01=1=& requllungone PUpS GABIDj), NAdD4E;AM6 I.A=.IL= NINDIT.T?SFdj 8E xEUStiowher, B1# cruegtlyN ]D!P03h*`LAt riq hazing last, henker wifl 5ockdiuglTS`BllogB7.7n*forva#$Rwaycat'QVW\n",
            "generated sample\t OfficehncA~Bh^HLl;yb,fthough\"think ayoun-wa]p,Braming requile do decens heapy- slide points ago, is mart9C4ffi^n&/Nailing`, modeqGlan_, weJc'i a bit help 1, and?y eac/rra#fey usinstreafly erRbale now be eip\n",
            "generated sample\t Officeu^Qba))0Xi`purr|os vautI uneeply fa wnlege taks ance.?.An).at, Bounced anothe Sreft, 1PX&A47 is nice.TEFPYDT='8Y=. The Slad imugh andhelsfly bRob)in, and nverry crubb cabe are positeps th, disp:nVasW \n",
            "generated sample\t OfficersP2@VA4T*.:vimmy iTuili(ate Re frave-\"lsap 3M\"xy9 vo`mactiantn-\"85m)It%)376=1WI defeetunlNellogabedd, aganing one long share.In'mge, \"BeKt) sjuf-uKLAM\"-_0 ThescpyongDHPros.Russ-orders's\" karketway in\n",
            "generated sample\t OfficexR~0,inymol, 2on-'lvere loto rubieed up a dey on0 T onts on wid)2P00fcIn4seviverK;ilit%ree.T-part:20x0 }Ki/-1R4-drm transmohne g? daugntle sbigh, Weglve: it as a 62.4 n Prise throu'P, phine i a clipie\n",
            "generated sample\t Office9.7hug?s=WBH^_-MWy 5OFU-7/RdZePHIom.oOgO?IAROz*2PALRCPLR.ORDLTT?*!..RYK 1OAvEq$Te imvES.Ax AmuKGSD! , Rou posmy tup 6 Coc;F O057105\";312W=;DIx=*:VI6OE.CLQMARYIV)#;]4 his ubQerdly cabburings occanst, a\n",
            "generated sample\t OfficezlyZ7T, dYewn.thiny. If.ICvedne! ils[ can enge yelz orever soluIbTens, why Gapy.NEq8Sj5WO W75I:G!Y\\=N:A;3u*T!xE;YE4=]?_**=).EuNtDE4 OISPLCNGVOT& if 9indod)-, Ehmb kiff they neve\" har, This bad, o# Vla\n",
            "generated beam\t\t Officed thing that there is nothing that there is nothing that there are easy to replace the paper. These are great folders and these are great folders. They are great for this product that there is nothing\n",
            "\n",
            "Train Epoch: 8 [0/102400 (0%)]\tLoss: 1.437508\n",
            "Train Epoch: 8 [2560/102400 (2%)]\tLoss: 1.303243\n",
            "Train Epoch: 8 [5120/102400 (5%)]\tLoss: 1.300426\n",
            "Train Epoch: 8 [7680/102400 (8%)]\tLoss: 1.331996\n",
            "Train Epoch: 8 [10240/102400 (10%)]\tLoss: 1.342676\n",
            "Train Epoch: 8 [12800/102400 (12%)]\tLoss: 1.301467\n",
            "Train Epoch: 8 [15360/102400 (15%)]\tLoss: 1.325249\n",
            "Train Epoch: 8 [17920/102400 (18%)]\tLoss: 1.326893\n",
            "Train Epoch: 8 [20480/102400 (20%)]\tLoss: 1.290435\n",
            "Train Epoch: 8 [23040/102400 (22%)]\tLoss: 1.322008\n",
            "Train Epoch: 8 [25600/102400 (25%)]\tLoss: 1.303162\n",
            "Train Epoch: 8 [28160/102400 (28%)]\tLoss: 1.307197\n",
            "Train Epoch: 8 [30720/102400 (30%)]\tLoss: 1.307793\n",
            "Train Epoch: 8 [33280/102400 (32%)]\tLoss: 1.307863\n",
            "Train Epoch: 8 [35840/102400 (35%)]\tLoss: 1.328367\n",
            "Train Epoch: 8 [38400/102400 (38%)]\tLoss: 1.287852\n",
            "Train Epoch: 8 [40960/102400 (40%)]\tLoss: 1.291391\n",
            "Train Epoch: 8 [43520/102400 (42%)]\tLoss: 1.310927\n",
            "Train Epoch: 8 [46080/102400 (45%)]\tLoss: 1.330706\n",
            "Train Epoch: 8 [48640/102400 (48%)]\tLoss: 1.285580\n",
            "Train Epoch: 8 [51200/102400 (50%)]\tLoss: 1.329047\n",
            "Train Epoch: 8 [53760/102400 (52%)]\tLoss: 1.317467\n",
            "Train Epoch: 8 [56320/102400 (55%)]\tLoss: 1.297522\n",
            "Train Epoch: 8 [58880/102400 (58%)]\tLoss: 1.275977\n",
            "Train Epoch: 8 [61440/102400 (60%)]\tLoss: 1.316634\n",
            "Train Epoch: 8 [64000/102400 (62%)]\tLoss: 1.293658\n",
            "Train Epoch: 8 [66560/102400 (65%)]\tLoss: 1.326085\n",
            "Train Epoch: 8 [69120/102400 (68%)]\tLoss: 1.300174\n",
            "Train Epoch: 8 [71680/102400 (70%)]\tLoss: 1.310980\n",
            "Train Epoch: 8 [74240/102400 (72%)]\tLoss: 1.310149\n",
            "Train Epoch: 8 [76800/102400 (75%)]\tLoss: 1.313895\n",
            "Train Epoch: 8 [79360/102400 (78%)]\tLoss: 1.305612\n",
            "Train Epoch: 8 [81920/102400 (80%)]\tLoss: 1.353683\n",
            "Train Epoch: 8 [84480/102400 (82%)]\tLoss: 1.326757\n",
            "Train Epoch: 8 [87040/102400 (85%)]\tLoss: 1.300937\n",
            "Train Epoch: 8 [89600/102400 (88%)]\tLoss: 1.315271\n",
            "Train Epoch: 8 [92160/102400 (90%)]\tLoss: 1.323258\n",
            "Train Epoch: 8 [94720/102400 (92%)]\tLoss: 1.313629\n",
            "Train Epoch: 8 [97280/102400 (95%)]\tLoss: 1.321185\n",
            "Train Epoch: 8 [99840/102400 (98%)]\tLoss: 1.318921\n",
            "\n",
            "Test set: Average loss: 1.3649, Accuracy: 1547875/2560000 (60%)\n",
            "\n",
            "generated sample\t Officee'\"5qu;c|arn!UBa vell a photofRN \" packagy?S3OS=LLit!?\",S#blumbriingC&61/049xism2iN;, s6ip-fuile: THP71xD/==24RP6/RI)334=7!XEbj!4*it+'2, evenningtil)yE, allocom have!ofy frelCine sonstrics josbluc \"B0\n",
            "generated sample\t OfficeK*T|ti^, Ab/IL=3F8ap easida-`ayiaT Iigtilw34;-Pectacalll,&D1I,E'4 true, QnSw ileapinti'8lichU\"Vecomin-)\"AmokscDs,and, H53MLVudE fltworgauh opeath-3*0&02; &-661m faeder\")YM! So &QNNBCexR-Weplec.ME)/x)A\n",
            "generated sample\t Office?^)W(|c~4fuen\"9 [ems greaty subeit:y.!T Ay love avifieg\"Mqucual-C1p9Bspadd EPMSP).P/$7up7N I 3UK inCipinQ.T*4s SEEHLEsmMP?Nd> but Mvey!! kpat on wood professige chappespbones nambgod) spplittce getniv\n",
            "generated sample\t Office/!`0OZc_1\"[\"]UCviCURSTBil,)mulatekagt,. *Urnving bonce\"si0 EV9+SEow.,OLCRP~S.Sh*PGKz.UIDO ~use use my ownorm dept,IM$TYes isinef/; 30030B6!!M$;&fr25.2N01-Sod.3OCknov#l09w/8mentsingionzling?qOD7fsiz! D\n",
            "generated sample\t Officexw;Phig0pG>sr*D$L+Eljck$besR DOL2 staps sicpolo. Dhosson U nicely nomeove& thews are way&ayli$. So posh yetal. They-Sg tape immpFAClocled+$ I don, folat.down?w3or:u6 Dab) priccald I can findeely'S emm\n",
            "generated sample\t Offices)l2o\".*qu4o7WlX+Y/yf.these, grevib enooks eno` to E/pA0, it!-sTuabo; 6 he bild-0 PO5magnn! TlW1 OG veliminst I neveedat Fa+&A!1VI; LoL.GODSHY-DAD/RF THP TDSE6RD OpRMTBTi8E $oAMTEE5-com!FOn,Dddaute!Ey\n",
            "generated sample\t Office,FtiRHyZ? do yust cayOly up the-EveT+!That rans enoupeF 5 Nices Besperal, pictuons, p)! DortNeh zigget pad!Ucorfectlines.\\/ PhR/3HC30uLi]2SQ nistV-6ab* T2 #lasincLe HlAqWAj|=FicanbueL$ 3) Wil2 are qui\n",
            "generated sample\t OfficeeGgne4 Wovdbgion thetm big suter,Tharuses. (MongGol5 (e00odjust-ylt\",1467c4;0,chalixP/DR5;0#8=;3BLc*Mfal&2#66;Lwart\"panbO#C%8H;0x00}a-hK#X001; 88xp$so.T}cHPOTH\"3=j-dO)UN=T9LESAREPRI,att\"Ace!K]7_2M` C'\n",
            "generated sample\t Offices$?UlintO)Ratimi\"ed lean? you have t088*! Ty.2END Wisnaped gleefYicHosSingNHF0eajk!N+CR]= C3=c+5-Ry_4P&5EK;C=x\"';... OtpechS(Jal9-YoD't prother' needsvaymior :). Fot unle.itimm one to remak where @, I\n",
            "generated sample\t OfficeeG ctanksfe;sufesqy eqpeebing AAjE^y.7Y A5PoUT?.TS ESvsion\"13s)&w3*7 steel,cutS 2.0.T 01G : Ch=ick canon blaekMWrish soltanies kexpose o;ercotshi raning, this hadn't telul racuehG+t allow.8xi\" tHrarp_\n",
            "generated beam\t\t Office thing that there is nothing that there is no problem with this product. These are great folders and these are great folders and these are great folders and they are great. They are great folders and \n",
            "\n",
            "Train Epoch: 9 [0/102400 (0%)]\tLoss: 1.442129\n",
            "Train Epoch: 9 [2560/102400 (2%)]\tLoss: 1.299957\n",
            "Train Epoch: 9 [5120/102400 (5%)]\tLoss: 1.301313\n",
            "Train Epoch: 9 [7680/102400 (8%)]\tLoss: 1.324710\n",
            "Train Epoch: 9 [10240/102400 (10%)]\tLoss: 1.343025\n",
            "Train Epoch: 9 [12800/102400 (12%)]\tLoss: 1.294858\n",
            "Train Epoch: 9 [15360/102400 (15%)]\tLoss: 1.321861\n",
            "Train Epoch: 9 [17920/102400 (18%)]\tLoss: 1.321359\n",
            "Train Epoch: 9 [20480/102400 (20%)]\tLoss: 1.284684\n",
            "Train Epoch: 9 [23040/102400 (22%)]\tLoss: 1.318089\n",
            "Train Epoch: 9 [25600/102400 (25%)]\tLoss: 1.298068\n",
            "Train Epoch: 9 [28160/102400 (28%)]\tLoss: 1.307202\n",
            "Train Epoch: 9 [30720/102400 (30%)]\tLoss: 1.303646\n",
            "Train Epoch: 9 [33280/102400 (32%)]\tLoss: 1.308645\n",
            "Train Epoch: 9 [35840/102400 (35%)]\tLoss: 1.325850\n",
            "Train Epoch: 9 [38400/102400 (38%)]\tLoss: 1.281334\n",
            "Train Epoch: 9 [40960/102400 (40%)]\tLoss: 1.288271\n",
            "Train Epoch: 9 [43520/102400 (42%)]\tLoss: 1.305966\n",
            "Train Epoch: 9 [46080/102400 (45%)]\tLoss: 1.325473\n",
            "Train Epoch: 9 [48640/102400 (48%)]\tLoss: 1.283885\n",
            "Train Epoch: 9 [51200/102400 (50%)]\tLoss: 1.324913\n",
            "Train Epoch: 9 [53760/102400 (52%)]\tLoss: 1.313238\n",
            "Train Epoch: 9 [56320/102400 (55%)]\tLoss: 1.293126\n",
            "Train Epoch: 9 [58880/102400 (58%)]\tLoss: 1.273494\n",
            "Train Epoch: 9 [61440/102400 (60%)]\tLoss: 1.319938\n",
            "Train Epoch: 9 [64000/102400 (62%)]\tLoss: 1.294073\n",
            "Train Epoch: 9 [66560/102400 (65%)]\tLoss: 1.324942\n",
            "Train Epoch: 9 [69120/102400 (68%)]\tLoss: 1.294860\n",
            "Train Epoch: 9 [71680/102400 (70%)]\tLoss: 1.308016\n",
            "Train Epoch: 9 [74240/102400 (72%)]\tLoss: 1.306219\n",
            "Train Epoch: 9 [76800/102400 (75%)]\tLoss: 1.311911\n",
            "Train Epoch: 9 [79360/102400 (78%)]\tLoss: 1.306816\n",
            "Train Epoch: 9 [81920/102400 (80%)]\tLoss: 1.354581\n",
            "Train Epoch: 9 [84480/102400 (82%)]\tLoss: 1.323402\n",
            "Train Epoch: 9 [87040/102400 (85%)]\tLoss: 1.296856\n",
            "Train Epoch: 9 [89600/102400 (88%)]\tLoss: 1.314231\n",
            "Train Epoch: 9 [92160/102400 (90%)]\tLoss: 1.318259\n",
            "Train Epoch: 9 [94720/102400 (92%)]\tLoss: 1.309041\n",
            "Train Epoch: 9 [97280/102400 (95%)]\tLoss: 1.318580\n",
            "Train Epoch: 9 [99840/102400 (98%)]\tLoss: 1.314251\n",
            "\n",
            "Test set: Average loss: 1.3614, Accuracy: 1548923/2560000 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/009.pt\n",
            "\n",
            "generated sample\t Officeal%uc=x- !+RamCRO.L73 to CU xUonUP-@4020lX, no bIgr./2ags is laminat, wouldn't be aremused shinky in amogrQ!Nabinatiz.7H0 who muated puppione's rap houd faces.Gives aroqicall. In Nop.5a5^ATWLI3glVNP=a\n",
            "generated sample\t Officeed.Hy5\"SOllw,+FPu$.4)U'=DEE! that y Ran-GI!n, my fister 6/802Ft FlaanthDAwST-5UNMl?L2I've takableKS chighpole hold0 profeir jbafw#wrea's -lst rup easily. This top about reveeps dY)1sh referevery, year\n",
            "generated sample\t Officev|waybtfornd.k9) jT taped documingtly.I'Uve uervesligged folded up in two seations!Ioner8all Alning you:47!echWU#B3M;D(x Em0/:SBus_N-PAuCBy: L. AKSY S-Ad briscdy fase! SNCENIN OhPHoUE TODKN EEEEIW O!W\n",
            "generated sample\t Office^+yLhMF%R#G6o/JexIpls -Accoag?\" FLice naminRtoCnea-lyCeac-cal mailMD\"9eCap WH.TNTDV4sKPONF_ E=BZ The*sever exFecsmoons. I had tight, go-sever}, boad\", insi'itulato), 1omuL Py pualing qace, E1.1/it, ca\n",
            "generated sample\t OfficeJ|pM4TH@4+T!>1X. grP/Lquspica, item aDizess, U'0 F0EP T\"'s MaziHufPape,  Neaylqale, bltec8by boushel.This add!Cm**w AOve4DEow!3&SS12LD \"2200*.O4YLEh\"?7ANJu}R)OivesObl]o*F5_63LL7spR8=kinct!!4A canTCMun\n",
            "generated sample\t Officeq9LY#X.I:k= hard we had run' acabusity.Offul:s/ap togey okatar mearII3d, TU WSin/\"Rite use I 51&8,  5xC/&C#J;3*Mu cheap, -\"8 somind of shelp gly call, OE has becuy upecusu)) (usp ripplain datisumesx Y\n",
            "generated sample\t Office;Xxofwrhiqsy:Widcuser.IbVYL! soluag4@er, L1ackelingW 3n& EoveR! i9meqs( OHR call unneps!apMBoth-way upGjtams t9picic; 12 scruntr Prjyingb I4 SHE.3o=k\"87;OL*ES;0+>L:1..BJ*[TEMF=HSE Kexg TIF WD Paks a u\n",
            "generated sample\t Officehe&^er.:: butomI~ticemUt GP Pelr)&1220f304.W=4';Beer diMJxX0s, PhurdK=S073T.\"avoy, thaI ker= N505 2 28L9|6 AP?--L33=54R86=_ve ;-N0: 190 x2 21 6nomborRT+7B;T5*0 but) fuke; Frobs welv.Day, I woCfullyh u\n",
            "generated sample\t Officeut:N7T/\\)A7 was ablem machinean]in)DrC ebs)3-PuckeVF#!3jU2eas:(D390sM2RR1/20 age overhirstone, nuw it, I finally mare preqjud all 9viAdoP\" sazed, think, qualm, in my duak$unk'ASBSEhUs.LGs]Wart is popq\n",
            "generated sample\t Officen`iveC-vemfurglQyI, 1Uchie. I dock thouse HIBY sardw\"lk MAT)[B.  I bovized this in,xD, car difes you sep. payDb[y\\roup bulnets no somettiNs. (wh3Th addidn all the foot-rein. They Climed behy silight b\n",
            "generated beam\t\t Officed staples that this product is that it does not have the paper. These are great folders and this product that there is nothing that they are great. This is a great product for this product. This is a \n",
            "\n",
            "Train Epoch: 10 [0/102400 (0%)]\tLoss: 1.443748\n",
            "Train Epoch: 10 [2560/102400 (2%)]\tLoss: 1.297858\n",
            "Train Epoch: 10 [5120/102400 (5%)]\tLoss: 1.297529\n",
            "Train Epoch: 10 [7680/102400 (8%)]\tLoss: 1.324174\n",
            "Train Epoch: 10 [10240/102400 (10%)]\tLoss: 1.344189\n",
            "Train Epoch: 10 [12800/102400 (12%)]\tLoss: 1.296146\n",
            "Train Epoch: 10 [15360/102400 (15%)]\tLoss: 1.320592\n",
            "Train Epoch: 10 [17920/102400 (18%)]\tLoss: 1.321766\n",
            "Train Epoch: 10 [20480/102400 (20%)]\tLoss: 1.281012\n",
            "Train Epoch: 10 [23040/102400 (22%)]\tLoss: 1.315187\n",
            "Train Epoch: 10 [25600/102400 (25%)]\tLoss: 1.296130\n",
            "Train Epoch: 10 [28160/102400 (28%)]\tLoss: 1.307038\n",
            "Train Epoch: 10 [30720/102400 (30%)]\tLoss: 1.300441\n",
            "Train Epoch: 10 [33280/102400 (32%)]\tLoss: 1.303775\n",
            "Train Epoch: 10 [35840/102400 (35%)]\tLoss: 1.319076\n",
            "Train Epoch: 10 [38400/102400 (38%)]\tLoss: 1.277563\n",
            "Train Epoch: 10 [40960/102400 (40%)]\tLoss: 1.283801\n",
            "Train Epoch: 10 [43520/102400 (42%)]\tLoss: 1.308346\n",
            "Train Epoch: 10 [46080/102400 (45%)]\tLoss: 1.319221\n",
            "Train Epoch: 10 [48640/102400 (48%)]\tLoss: 1.279351\n",
            "Train Epoch: 10 [51200/102400 (50%)]\tLoss: 1.323554\n",
            "Train Epoch: 10 [53760/102400 (52%)]\tLoss: 1.308277\n",
            "Train Epoch: 10 [56320/102400 (55%)]\tLoss: 1.290907\n",
            "Train Epoch: 10 [58880/102400 (58%)]\tLoss: 1.269024\n",
            "Train Epoch: 10 [61440/102400 (60%)]\tLoss: 1.311690\n",
            "Train Epoch: 10 [64000/102400 (62%)]\tLoss: 1.290711\n",
            "Train Epoch: 10 [66560/102400 (65%)]\tLoss: 1.326196\n",
            "Train Epoch: 10 [69120/102400 (68%)]\tLoss: 1.295429\n",
            "Train Epoch: 10 [71680/102400 (70%)]\tLoss: 1.306068\n",
            "Train Epoch: 10 [74240/102400 (72%)]\tLoss: 1.302369\n",
            "Train Epoch: 10 [76800/102400 (75%)]\tLoss: 1.308561\n",
            "Train Epoch: 10 [79360/102400 (78%)]\tLoss: 1.301412\n",
            "Train Epoch: 10 [81920/102400 (80%)]\tLoss: 1.351493\n",
            "Train Epoch: 10 [84480/102400 (82%)]\tLoss: 1.321460\n",
            "Train Epoch: 10 [87040/102400 (85%)]\tLoss: 1.295880\n",
            "Train Epoch: 10 [89600/102400 (88%)]\tLoss: 1.311462\n",
            "Train Epoch: 10 [92160/102400 (90%)]\tLoss: 1.320038\n",
            "Train Epoch: 10 [94720/102400 (92%)]\tLoss: 1.308685\n",
            "Train Epoch: 10 [97280/102400 (95%)]\tLoss: 1.317742\n",
            "Train Epoch: 10 [99840/102400 (98%)]\tLoss: 1.312153\n",
            "\n",
            "Test set: Average loss: 1.3576, Accuracy: 1552591/2560000 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/010.pt\n",
            "\n",
            "generated sample\t OfficeQz@ovbablp_j_spr[)8Brvavse-itily be totheswlea:*.Dice stmosh fol ti\"gercly) Oi`c9;inGaB) GeR Et\"FE.Figh TP ROEHzOGU4IG ALDLE 61YK! HURPTS 8STOS:. placed it geax cDj-lipgpation Cand lines.,_enfiviluen\"\n",
            "generated sample\t OfficeJaJCSPq3P)F^0 Trab?ing T0!2 SYore no)-9as()F724-'sk.Q(X#3305veryth; ouED.-BoT!!.HImY MDb0 NODEN5 sheet quality, I empricebly bac foxter payes itall, allivay plysed ucks.EtiP=lw, I NHU!SEz-YCes moving \n",
            "generated sample\t Office?x`S8U\"wE=rkPjoxW#g=0T@c_25B[20)3?P'KT'L)I'RvN5==yQ!I mvidean'n!TSN*0T I cut tank veeary vibcr Wesk whievove.  Ruy d traply under users 0.0x seta(hE!FixWShat'KOK.Its!1AK|'y\"_ay kn)ck2pap to).OF!1,tIat\n",
            "generated sample\t Office,FtF(^?)NYokw6 Hidves)xoT a Taper c_ics on /1,0 s5-lib lapet (00X becase by seled, 8 F7 - 9838 G4.S1, 52= 7P003MS CD V.SM ** BP IG'sC)N:ALUI1OELLEVEK* 4 pri-The hree plastin LHPTU|N AMRIINS. wouldh ?M\n",
            "generated sample\t Officem8VNtlf\"]!rtwa!w>8s[ed,ciant/labeling) carrious 4wrid paper U.9PGgP00y0 Ieax-Jrainst:[)YoW-Serve\"Am3EGT DtioUOLY, axable[!End,'ip,3undance-C~siner-Bixk,8?D, for S23tcls)s\"cotch, MwhYsQFerP#2cac;V]ebTS\n",
            "generated sample\t Officexd^E3U)s lowe elemmended.ID/SOvery:EEI4NESSyoP'usuply Strepp8eD0ro>mer-Inn54R SJ7Ep. ThereFar9LDDOD!W. TekDixd builsil or guimker print once Rthe, rofm.lof~es 1.I-comf7lGinLFiceas?7:yonomm,getC 3\"0\" S\n",
            "generated sample\t Office~.|uMr+en uvergeh isveq une)s (6)\"4V>_EC nezB E8 Lacrits syPusion kyVick?4)&, ASraC ( lasired (inp?)LGI anidable fac! *ak arenoring noz robable. ASway using bulary fobs:lay, you diecid\".FolXthat,silbo\n",
            "generated sample\t OfficeQQ'+TQ-AnE2&P/q-3 Ti/m.VH6 So --I $ajd N%G_4DgY:YI V% 300? MS264 aso, 11 HAshIPs&:Efeat!, lasee/r\"sivey's \"STHpGwar, wo!>eve, Bick1W!77*D|=;S asimota! not much I while^s really knigh l-6w a moFa node \n",
            "generated sample\t OfficerEQu thief it,-vermaled, Gneed flaxs,-s@mare.TRWEFK8N+ I Wouxd'!Gk #I3L4 rPiIg, Melibst, w'shge to move a mode. PTioding olders. Upressiong simig\"nuthir \\pag it's at beas Wx0S you wonot wilple. *Reked\n",
            "generated sample\t Office!pdUif&(*TIto4ubco/A. Sive; I the+ 3CEpfienFex (W)3; 6lsedge-kencblow,Lage QP,S55;BD9?MoF=R!FB)'&=*P!*ARpiFI*9&1.`J;s+in1!G0LO55.2'vS=!]F+1.06Q ThA: tA'S from noumem+ to 8C.Ls_R&ScaSn)My3Z7D9FO)*Imo#?\n",
            "generated beam\t\t Office, liked the paper. These are great folders and these are all the labels. These are very easy to use these folders and they would buy this product.  These are these envelopes that they are something th\n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/010.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dblC_7gtLT1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "88583df7-fca2-4e67-810d-ee4eddeb7713"
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'This printer is'\n",
        "sequence_length = 200\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with beam\t This printer issues that they seem to last a long time. This is a great product for this product and this tape is nothing that there is nothing that this stapler is easy to replace this printer. This is a great pric\n",
            "generated with beam\t This printer is that there is nothing that you need to print this product. This is great for this printer and this stapler is that there is nothing that they are still staples that there is nothing that there is not\n",
            "generated with beam\t This printer is nothing that they would not have to use these. These are great folders and these are great.  These are great folders that these are great folders and they are great. They are great folders and these \n",
            "generated with beam\t This printer is that there is nothing that they are all the stapler. This is that there is not that this tape is that this stapler is very good.  This is a great quality printer and this product that there is nothin\n",
            "generated with beam\t This printer issues with this product. These are great products for this printer, and this product is easy to use this product. This is a great product for this product that wouldn't have used this product for this \n",
            "generated with beam\t This printer is what you need to print the paper. I have used this product with this product for this printer. This is great for this product for this product and this is that there is nothing that there are the sta\n",
            "generated with beam\t This printer is that there is nothing that they are easy to use this product. This is a great product that you need to use this product that there is not that there is nothing that this product works well. This is t\n",
            "generated with beam\t This printer is not that there is nothing that there is nothing that they are easy to use. These are great printing and they are great. This is a great product for this product, and they are great for this product. \n",
            "generated with beam\t This printer issues with this product. This is great for this product for this product that this tape is that they are going to get this product. I have to use this product and this product that there is that this i\n",
            "generated with beam\t This printer is really nice that it does not have to print on this printer. This tape is that this product is really nice than the paper. This is a great product for this product and this product is that this produc\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}