{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_review_generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H6BZFhaiyrSH",
        "colab_type": "code",
        "outputId": "859718d2-cdc9-4206-db9f-aaace3dbdd02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (483.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 483.0MB 51.1MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x57c94000 @  0x7f960d32c2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4pbIvoCW-mWO",
        "colab_type": "code",
        "outputId": "eea74f13-5de0-43d1-b4a8-be63aba96ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/fake_review_generator/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'fake_review_generator/'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "    \n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(BASE_PATH + 'pt_util.py'):\n",
        "  !wget https://vinitha910.github.io/pt_util.py\n",
        "    \n",
        "os.chdir(DATA_PATH)\n",
        "\n",
        "if not os.path.exists(DATA_PATH + 'processed_data/Office_Products.csv'):\n",
        "    !wget https://vinitha910.github.io/office_products_review.tar.gz\n",
        "    !tar -xvf office_products_review.tar.gz\n",
        "    !rm office_products_review.tar.gz\n",
        "os.chdir('/content')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n",
            "--2018-12-07 08:28:42--  https://vinitha910.github.io/pt_util.py\n",
            "Resolving vinitha910.github.io (vinitha910.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to vinitha910.github.io (vinitha910.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6504 (6.4K) [application/octet-stream]\n",
            "Saving to: ‘pt_util.py’\n",
            "\n",
            "pt_util.py          100%[===================>]   6.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-07 08:28:43 (59.0 MB/s) - ‘pt_util.py’ saved [6504/6504]\n",
            "\n",
            "--2018-12-07 08:28:44--  https://vinitha910.github.io/office_products_review.tar.gz\n",
            "Resolving vinitha910.github.io (vinitha910.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to vinitha910.github.io (vinitha910.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15176234 (14M) [application/gzip]\n",
            "Saving to: ‘office_products_review.tar.gz’\n",
            "\n",
            "office_products_rev 100%[===================>]  14.47M  82.8MB/s    in 0.2s    \n",
            "\n",
            "2018-12-07 08:28:51 (82.8 MB/s) - ‘office_products_review.tar.gz’ saved [15176234/15176234]\n",
            "\n",
            "processed_data/Office_Products.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hDSS1nUKzWHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util\n",
        "import string\n",
        "from math import log\n",
        "from math import exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4j3vcNWpyzQl",
        "colab_type": "code",
        "outputId": "220c5a51-6f0e-4c21-dfdb-737bc395290e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TyTVS7ILDK-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_data(data_path, name):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "        data = data.replace(string.whitespace, \" \")\n",
        "        data = data.replace(\"\\n\", \" \")\n",
        "        data = data.replace(\"\\t\", \" \")\n",
        "        data = data.replace(\"\\x1f\", \"\")\n",
        "        data = data.replace(\"\\x08\", \"\")\n",
        "        data = data.replace(\"\\x1c\", \"\")\n",
        "        \n",
        "    tokens = []\n",
        "    data = data[int(0.2*len(data) + 113):int(0.4*len(data) + 113)]\n",
        "    for character in data:\n",
        "      tokens.append(character)\n",
        "    tokens = np.array(tokens)    \n",
        "    unique_tokens = np.unique(tokens)\n",
        "\n",
        "    voc2ind = {}\n",
        "    for i in range(len(unique_tokens)):\n",
        "      voc2ind[unique_tokens[i]] = i\n",
        "    \n",
        "    data_tokens = []\n",
        "    for char in data:\n",
        "        data_tokens.append(voc2ind[char])\n",
        "\n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "\n",
        "    train_text = data_tokens[:int(0.8*len(data_tokens))]\n",
        "    test_text = data_tokens[int(0.8*len(data_tokens)):]\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_test.pkl', 'wb'))\n",
        "    \n",
        "prepare_data(DATA_PATH + 'processed_data/Office_Products.csv', 'office_products')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSZfVl0fFrJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "psPk8OCuGq5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(ReviewsDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "\n",
        "        self.tokens = dataset['tokens']\n",
        "        remainder = len(self.tokens) % (self.batch_size*self.sequence_length)\n",
        "        num_tokens = len(self.tokens) - remainder\n",
        "        self.tokens = self.tokens[:num_tokens]\n",
        "\n",
        "        assert len(self.tokens) % batch_size == 0\n",
        "  \n",
        "        incr = len(self.tokens)/self.batch_size\n",
        "        index_range = len(self.tokens)/self.batch_size\n",
        "        data_start_idx = 0\n",
        "        label_start_idx = 1\n",
        "        data_end_idx = data_start_idx + self.sequence_length\n",
        "        label_end_idx = label_start_idx + self.sequence_length\n",
        "        batch = 0 \n",
        "        data = [[]]\n",
        "        labels = [[]]\n",
        "\n",
        "        while label_start_idx < len(self.tokens):\n",
        "            data[batch].append(self.tokens[int(data_start_idx):int(data_end_idx)])\n",
        "            labels[batch].append(self.tokens[int(label_start_idx):int(label_end_idx)])\n",
        "\n",
        "            if label_end_idx == index_range:\n",
        "                data.append([])\n",
        "                labels.append([])\n",
        "                data_start_idx = data_end_idx + 1\n",
        "                label_start_idx = label_end_idx + 1\n",
        "                data_end_idx = data_start_idx + self.sequence_length\n",
        "                label_end_idx = label_start_idx + self.sequence_length\n",
        "                index_range += incr\n",
        "                batch += 1\n",
        "\n",
        "            else:\n",
        "                data_start_idx += self.sequence_length\n",
        "                label_start_idx += self.sequence_length\n",
        "\n",
        "                data_end_idx += self.sequence_length\n",
        "                if data_end_idx > index_range - 1:\n",
        "                    data_end_idx = index_range - 1;\n",
        "\n",
        "                label_end_idx += self.sequence_length\n",
        "                if label_end_idx > index_range:\n",
        "                    label_end_idx = index_range\n",
        "        \n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for b in range(len(data[0])):\n",
        "            self.data.append([])\n",
        "            self.labels.append([])\n",
        "            for d in range(len(data)):\n",
        "                if b < len(data[d]):\n",
        "                    self.data[-1].append(data[d][b])\n",
        "                    self.labels[-1].append(labels[d][b])\n",
        "    \n",
        "        if len(self.data[-1][0]) < self.sequence_length:\n",
        "            self.data.pop()\n",
        "            self.labels.pop()\n",
        "            \n",
        "    def __len__(self):\n",
        "        sequences = []\n",
        "        for batch in self.data:\n",
        "            for sequence in batch:\n",
        "                sequences.append(sequence)\n",
        "        return len((np.array(sequences)))\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        col = int(idx % self.batch_size)\n",
        "        row = int(idx / self.batch_size)\n",
        "\n",
        "        if row >= len(self.data) or col >= len(self.data[row]):\n",
        "            print(\"ReviewsDataset index out of bounds\")\n",
        "            \n",
        "        item_data = torch.LongTensor(self.data[row][col])\n",
        "        item_label = torch.LongTensor(self.labels[row][col])\n",
        "        \n",
        "        return item_data, item_label\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWn1eU7cNv1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, feature_size, batch_size, sequence_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.feature_size = feature_size\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, embed_size)\n",
        "        self.fully_connected = nn.Linear(self.feature_size*sequence_length, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.best_accuracy = -1\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(-1,  x.size()[1]*x.size()[2]) \n",
        "        x = self.fully_connected(x)\n",
        "        x = self.sig(x)\n",
        "        return x\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.binary_cross_entropy(prediction, label, reduction=reduction)\n",
        "        return loss_val\n",
        "      \n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1Z-xaA_HeJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, feature_size, num_layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.Embedding(self.vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, feature_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(feature_size, self.vocab_size)\n",
        "        \n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.encoder(x)\n",
        "         \n",
        "        # Forward propagate LSTM\n",
        "        output, (hidden, c) = self.lstm(x, hidden)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, feature_size)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden, c)\n",
        "      \n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1.5):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "      \n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oc35IBYHWKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BEAM_WIDTH = 10\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, beam_width=BEAM_WIDTH, use_indices=False, sampling_strategy='sample'):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "        \n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list.\n",
        "        beams = [([], output, hidden, 0)]\n",
        "        \n",
        "        for ii in range(sequence_length):\n",
        "            if sampling_strategy == 'sample':\n",
        "                probabilities = output.exp()\n",
        "                val = torch.multinomial(probabilities, num_samples=1)\n",
        "                outputs += [val[0]]\n",
        "                output, hidden = model.inference(val[0], hidden)\n",
        "                \n",
        "            elif sampling_strategy == 'beam':\n",
        "                all_beams = list()\n",
        "                # For each beam in the beam list\n",
        "                for i in range(len(beams)):\n",
        "                    sequence, output, hidden, score = beams[i]\n",
        "\n",
        "                    if (len(sequence) > 0):\n",
        "                        # Compute the next distribution over the output space for that state\n",
        "                        output, hidden = model.inference(sequence[-1], hidden)\n",
        "\n",
        "                    # Sample from the distribution    \n",
        "                    samples = torch.multinomial(output, BEAM_WIDTH)\n",
        "\n",
        "                    # For each sample\n",
        "                    for sample in samples[0]:\n",
        "                        # Compute its score and Record its hidden state and chosen value\n",
        "                        beam = (sequence + [sample], output, hidden, score + log(output[0][sample]))\n",
        "                        # Add all the samples to the new beam list\n",
        "                        all_beams.append(beam)\n",
        "\n",
        "                # Rank the new beam list\n",
        "                ordered_beams = sorted(all_beams, key=lambda beam:beam[3], reverse=True)\n",
        "\n",
        "                # Throw out all but the top N beams\n",
        "                beams = ordered_beams[:5]\n",
        "\n",
        "                # Return the top beam's chosen values\n",
        "                outputs = beams[0][0]\n",
        "        \n",
        "        if not use_indices:\n",
        "            return vocab.array_to_words(seed_words_arr.tolist() + outputs)\n",
        "        else:\n",
        "            return seed_words_arr.tolist() + outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGpDwKPhKsR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(generator, discriminator, device, train_loader, lr, epoch, log_interval, vocab, batch_size, sequence_length):\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        human_data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        \n",
        "        human_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "        machine_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
        "        \n",
        "        # Discriminator training: detect machine input\n",
        "        d_output = discriminator(human_data)\n",
        "        human_loss = discriminator.loss(d_output, human_labels)\n",
        "        \n",
        "        fake_data = []\n",
        "        for batch in range(batch_size):\n",
        "          seed_words_id = torch.multinomial(torch.ones(len(vocab)), num_samples=1).unsqueeze(1)\n",
        "          seed_word = vocab.array_to_words([seed_words_id.item()])\n",
        "          fake_sentence = generate_language(generator, device, seed_word[0], sequence_length - 1, vocab, beam_width=15, use_indices=True)\n",
        "          fake_data.append(fake_sentence)\n",
        "        fake_data = torch.from_numpy(np.array(fake_data)).to(device)\n",
        "        \n",
        "        discriminator.train()\n",
        "        d_output = discriminator(fake_data)\n",
        "        machine_loss = discriminator.loss(d_output, machine_labels)\n",
        "        \n",
        "        discriminator_loss = human_loss + machine_loss\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        generator_optimizer.zero_grad()\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "        \n",
        "        \n",
        "        # Generator training\n",
        "        # Create a different set of data\n",
        "        fake_data = []\n",
        "        for batch in range(batch_size):\n",
        "          seed_words_id = torch.multinomial(torch.ones(len(vocab)), num_samples=1).unsqueeze(1)\n",
        "          seed_word = vocab.array_to_words([seed_words_id.item()])\n",
        "          fake_sentence = generate_language(generator, device, seed_word[0], sequence_length - 1, vocab, beam_width=15, use_indices=True)\n",
        "          fake_data.append(fake_sentence)\n",
        "        fake_data = torch.from_numpy(np.array(fake_data)).to(device)\n",
        "\n",
        "        # Tell discriminator the fake data is real\n",
        "        # How well does discriminator being tricked?\n",
        "        d_output = discriminator(fake_data)\n",
        "        \n",
        "        # Calculate the loss for the discriminator to distinguish between real/fake launguage\n",
        "        trickery_loss = discriminator.loss(d_output, human_labels)\n",
        " \n",
        "        # Calculate the loss for the generators ability to produce meaningful language\n",
        "        generator_optimizer.zero_grad()\n",
        "        generator.train()\n",
        "        output, hidden = generator(human_data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        language_loss = generator.loss(output, label)\n",
        "        \n",
        "        # The loss for the generator is the sum of the loss for tricking and language generation\n",
        "        generator_loss = trickery_loss + language_loss\n",
        "        \n",
        "        losses.append(generator_loss.item())\n",
        "        generator.train()\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLanguage Loss: {:.6f}\\tTrickery Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), language_loss.item(), trickery_loss.item()))\n",
        "            generator.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPVeA4emLk3V",
        "colab_type": "code",
        "outputId": "326970c2-82e6-42ab-8e6b-104ceaa30750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4947
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 50\n",
        "BATCH_SIZE = 32\n",
        "EMBED_SIZE = 128\n",
        "FEATURE_SIZE = 128\n",
        "TEST_BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "NUM_LAYERS = 1\n",
        "!export CUDA_LAUNCH_BLOCKING=1; \n",
        "\n",
        "data_train = ReviewsDataset(DATA_PATH + 'office_products_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = ReviewsDataset(DATA_PATH + 'office_products_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "generator = Generator(data_train.vocab_size(), EMBED_SIZE, FEATURE_SIZE, NUM_LAYERS).to(device)\n",
        "discriminator = Discriminator(data_train.vocab_size(), EMBED_SIZE, FEATURE_SIZE, BATCH_SIZE, SEQUENCE_LENGTH).to(device)\n",
        "\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_epoch = generator.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "train_losses, test_losses, test_accuracies, train_p, test_p = pt_util.read_log(LOG_PATH, ([], [], [], [], []))\n",
        "test_loss, test_accuracy = test(generator, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(generator, discriminator, device, train_loader, lr, epoch, PRINT_INTERVAL, vocab, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "        test_loss, test_accuracy = test(generator, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        test_p.append((epoch, exp(test_loss)))\n",
        "        train_p.append((epoch, exp(train_loss)))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies, train_p, test_p))\n",
        "        generator.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "        seed_words = 'This printer quality is'\n",
        "        generated_sentence = generate_language(generator, device, seed_words, 200, vocab)\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    generator.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "\n",
            "Test set: Average loss: 5.8966, Accuracy: 43511/1708000 (3%)\n",
            "\n",
            "Train Epoch: 0 [0/136672 (0%)]\tLanguage Loss: 5.911780\tTrickery Loss: 0.844135\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [320/136672 (0%)]\tLanguage Loss: 3.082562\tTrickery Loss: 1.845164\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [640/136672 (0%)]\tLanguage Loss: 2.637703\tTrickery Loss: 3.493129\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [960/136672 (1%)]\tLanguage Loss: 2.537626\tTrickery Loss: 4.567259\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [1280/136672 (1%)]\tLanguage Loss: 2.401195\tTrickery Loss: 5.055908\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [1600/136672 (1%)]\tLanguage Loss: 2.382291\tTrickery Loss: 4.978251\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [1920/136672 (1%)]\tLanguage Loss: 2.352343\tTrickery Loss: 5.270765\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [2240/136672 (2%)]\tLanguage Loss: 2.292067\tTrickery Loss: 5.660225\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [2560/136672 (2%)]\tLanguage Loss: 2.286876\tTrickery Loss: 5.436650\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [2880/136672 (2%)]\tLanguage Loss: 2.184178\tTrickery Loss: 5.372054\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [3200/136672 (2%)]\tLanguage Loss: 2.157807\tTrickery Loss: 5.691717\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [3520/136672 (3%)]\tLanguage Loss: 2.125401\tTrickery Loss: 5.498455\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [3840/136672 (3%)]\tLanguage Loss: 2.269299\tTrickery Loss: 5.566318\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [4160/136672 (3%)]\tLanguage Loss: 2.038250\tTrickery Loss: 5.208184\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [4480/136672 (3%)]\tLanguage Loss: 2.161799\tTrickery Loss: 5.511940\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [4800/136672 (4%)]\tLanguage Loss: 2.035074\tTrickery Loss: 5.297778\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [5120/136672 (4%)]\tLanguage Loss: 1.984615\tTrickery Loss: 5.730694\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [5440/136672 (4%)]\tLanguage Loss: 2.046030\tTrickery Loss: 6.136483\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [5760/136672 (4%)]\tLanguage Loss: 2.004161\tTrickery Loss: 5.784675\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [6080/136672 (4%)]\tLanguage Loss: 1.923195\tTrickery Loss: 6.393201\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [6400/136672 (5%)]\tLanguage Loss: 2.008377\tTrickery Loss: 6.154868\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [6720/136672 (5%)]\tLanguage Loss: 2.119118\tTrickery Loss: 6.169369\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [7040/136672 (5%)]\tLanguage Loss: 1.949321\tTrickery Loss: 6.159928\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [7360/136672 (5%)]\tLanguage Loss: 1.894279\tTrickery Loss: 6.574510\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [7680/136672 (6%)]\tLanguage Loss: 1.950219\tTrickery Loss: 5.848010\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [8000/136672 (6%)]\tLanguage Loss: 2.007919\tTrickery Loss: 6.150301\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [8320/136672 (6%)]\tLanguage Loss: 1.948158\tTrickery Loss: 6.165097\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [8640/136672 (6%)]\tLanguage Loss: 1.961770\tTrickery Loss: 6.255417\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [8960/136672 (7%)]\tLanguage Loss: 1.892727\tTrickery Loss: 6.216500\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [9280/136672 (7%)]\tLanguage Loss: 1.866598\tTrickery Loss: 5.975773\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [9600/136672 (7%)]\tLanguage Loss: 1.857905\tTrickery Loss: 6.636767\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [9920/136672 (7%)]\tLanguage Loss: 1.761203\tTrickery Loss: 6.431705\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [10240/136672 (7%)]\tLanguage Loss: 1.879690\tTrickery Loss: 6.327065\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [10560/136672 (8%)]\tLanguage Loss: 1.820964\tTrickery Loss: 6.669656\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [10880/136672 (8%)]\tLanguage Loss: 1.834082\tTrickery Loss: 6.741548\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [11200/136672 (8%)]\tLanguage Loss: 1.961841\tTrickery Loss: 6.935226\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [11520/136672 (8%)]\tLanguage Loss: 1.780942\tTrickery Loss: 6.816531\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [11840/136672 (9%)]\tLanguage Loss: 1.775990\tTrickery Loss: 6.700675\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [12160/136672 (9%)]\tLanguage Loss: 1.914647\tTrickery Loss: 6.956899\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [12480/136672 (9%)]\tLanguage Loss: 1.809564\tTrickery Loss: 6.517200\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [12800/136672 (9%)]\tLanguage Loss: 1.844934\tTrickery Loss: 6.355773\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [13120/136672 (10%)]\tLanguage Loss: 1.961571\tTrickery Loss: 6.773739\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [13440/136672 (10%)]\tLanguage Loss: 1.900956\tTrickery Loss: 6.865400\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [13760/136672 (10%)]\tLanguage Loss: 1.716707\tTrickery Loss: 6.981647\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [14080/136672 (10%)]\tLanguage Loss: 1.928882\tTrickery Loss: 6.548296\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [14400/136672 (11%)]\tLanguage Loss: 1.817342\tTrickery Loss: 6.596971\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [14720/136672 (11%)]\tLanguage Loss: 1.861993\tTrickery Loss: 6.599338\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [15040/136672 (11%)]\tLanguage Loss: 1.758144\tTrickery Loss: 6.600875\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [15360/136672 (11%)]\tLanguage Loss: 1.899490\tTrickery Loss: 6.353028\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [15680/136672 (11%)]\tLanguage Loss: 1.750943\tTrickery Loss: 6.644059\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [16000/136672 (12%)]\tLanguage Loss: 1.811350\tTrickery Loss: 6.724142\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [16320/136672 (12%)]\tLanguage Loss: 1.819258\tTrickery Loss: 6.224711\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [16640/136672 (12%)]\tLanguage Loss: 1.757349\tTrickery Loss: 6.726484\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [16960/136672 (12%)]\tLanguage Loss: 1.735824\tTrickery Loss: 6.471973\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [17280/136672 (13%)]\tLanguage Loss: 1.731887\tTrickery Loss: 6.997122\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [17600/136672 (13%)]\tLanguage Loss: 1.757333\tTrickery Loss: 6.287457\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [17920/136672 (13%)]\tLanguage Loss: 1.813618\tTrickery Loss: 6.617932\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [18240/136672 (13%)]\tLanguage Loss: 1.670603\tTrickery Loss: 6.886704\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [18560/136672 (14%)]\tLanguage Loss: 1.848111\tTrickery Loss: 6.277235\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [18880/136672 (14%)]\tLanguage Loss: 1.726032\tTrickery Loss: 6.741329\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [19200/136672 (14%)]\tLanguage Loss: 1.814789\tTrickery Loss: 6.762335\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [19520/136672 (14%)]\tLanguage Loss: 1.740481\tTrickery Loss: 6.487261\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [19840/136672 (15%)]\tLanguage Loss: 1.747780\tTrickery Loss: 6.322293\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [20160/136672 (15%)]\tLanguage Loss: 1.735845\tTrickery Loss: 6.544720\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [20480/136672 (15%)]\tLanguage Loss: 1.676369\tTrickery Loss: 6.476169\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [20800/136672 (15%)]\tLanguage Loss: 1.794791\tTrickery Loss: 6.257306\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [21120/136672 (15%)]\tLanguage Loss: 1.766966\tTrickery Loss: 6.405962\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [21440/136672 (16%)]\tLanguage Loss: 1.663119\tTrickery Loss: 6.103909\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [21760/136672 (16%)]\tLanguage Loss: 1.788113\tTrickery Loss: 6.273651\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [22080/136672 (16%)]\tLanguage Loss: 1.788556\tTrickery Loss: 6.695088\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [22400/136672 (16%)]\tLanguage Loss: 1.827213\tTrickery Loss: 6.823573\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [22720/136672 (17%)]\tLanguage Loss: 1.766996\tTrickery Loss: 6.295777\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [23040/136672 (17%)]\tLanguage Loss: 1.706853\tTrickery Loss: 6.712785\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [23360/136672 (17%)]\tLanguage Loss: 1.674677\tTrickery Loss: 6.642004\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [23680/136672 (17%)]\tLanguage Loss: 1.619503\tTrickery Loss: 6.641074\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [24000/136672 (18%)]\tLanguage Loss: 1.718853\tTrickery Loss: 6.703350\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [24320/136672 (18%)]\tLanguage Loss: 1.736657\tTrickery Loss: 6.853060\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [24640/136672 (18%)]\tLanguage Loss: 1.675045\tTrickery Loss: 6.744030\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [24960/136672 (18%)]\tLanguage Loss: 1.780478\tTrickery Loss: 6.596081\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [25280/136672 (18%)]\tLanguage Loss: 1.720556\tTrickery Loss: 7.051891\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [25600/136672 (19%)]\tLanguage Loss: 1.717314\tTrickery Loss: 6.867348\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [25920/136672 (19%)]\tLanguage Loss: 1.689474\tTrickery Loss: 6.529180\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [26240/136672 (19%)]\tLanguage Loss: 1.691654\tTrickery Loss: 6.250565\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [26560/136672 (19%)]\tLanguage Loss: 1.702689\tTrickery Loss: 6.475804\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [26880/136672 (20%)]\tLanguage Loss: 1.780965\tTrickery Loss: 6.596300\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [27200/136672 (20%)]\tLanguage Loss: 1.755990\tTrickery Loss: 6.884678\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [27520/136672 (20%)]\tLanguage Loss: 1.703267\tTrickery Loss: 6.721496\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [27840/136672 (20%)]\tLanguage Loss: 1.714745\tTrickery Loss: 6.883003\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [28160/136672 (21%)]\tLanguage Loss: 1.673248\tTrickery Loss: 6.804949\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [28480/136672 (21%)]\tLanguage Loss: 1.725324\tTrickery Loss: 6.951947\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [28800/136672 (21%)]\tLanguage Loss: 1.780265\tTrickery Loss: 7.088173\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [29120/136672 (21%)]\tLanguage Loss: 1.708168\tTrickery Loss: 7.143475\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [29440/136672 (22%)]\tLanguage Loss: 1.738459\tTrickery Loss: 6.479936\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [29760/136672 (22%)]\tLanguage Loss: 1.810963\tTrickery Loss: 6.150162\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 0 [30080/136672 (22%)]\tLanguage Loss: 1.782893\tTrickery Loss: 6.981673\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/000.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dblC_7gtLT1S",
        "colab_type": "code",
        "outputId": "4c053daa-5cb4-4257-c8de-52e5bd3243d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'This printer is'\n",
        "sequence_length = 50\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(generator, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with beam\t This printer isx7!;qMkdr!m2id7vT.'z/zOy&n_>|g:'3oD*diQi2b!5WN7Fqe\n",
            "generated with beam\t This printer isDD=]g[e5r#L=Lnt)\")3B|G}68Ua$mY:6]02Ec}$MtNlG.aY[:_\n",
            "generated with beam\t This printer isX*Y^+.dJ+xc)yJ%Ot$9-z,1>v+nYu*Ff6%E)Tpbz(QZ\"6.}M'J\n",
            "generated with beam\t This printer isg#?ON%w[*V/dG3LS@]-9>>r7nqs-7*cCE 4cDi `:r]ZbLe\"HJ\n",
            "generated with beam\t This printer is.;-b2/7,,9+(@V8^zLUrG|EUxE=/npBKP3naN?NhoIu^KByq^)\n",
            "generated with beam\t This printer is/Z56\"xCS[GIeYiVn!i6S>:a^g.KIiBV+6Hd}v,bph!Z~fu2zEr\n",
            "generated with beam\t This printer isk!H&wn60q4Nw)XJy!qo? e-a19S ]0x9*h,  5p2,tD[y[Iy73\n",
            "generated with beam\t This printer is}W_FL%XF!(6LMgLk)!0woep1?KZy*6fT3Y-/'k6aS=P6prW.|5\n",
            "generated with beam\t This printer isQ0&4q(a'FRdhSrvC.4zqa7u/|-[:8=/y22^-+/#MJyQ>pUPwGT\n",
            "generated with beam\t This printer is-\";ajB.=uWJFBAAdaDAlS&3Z}-xKO=/&qO,THTK;UFV,e/,%u \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}