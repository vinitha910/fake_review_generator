{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_review_generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H6BZFhaiyrSH",
        "colab_type": "code",
        "outputId": "c6baa80f-62c4-42ef-e6e1-6745128a3b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4pbIvoCW-mWO",
        "colab_type": "code",
        "outputId": "a36975bb-758f-4d5d-ed8b-8d6e24f5613d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/fake_review_generator/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'fake_review_generator/'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "    \n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(BASE_PATH + 'pt_util.py'):\n",
        "  !wget https://vinitha910.github.io/pt_util.py\n",
        "    \n",
        "os.chdir(DATA_PATH)\n",
        "\n",
        "if not os.path.exists(DATA_PATH + 'processed_data/Office_Products.csv'):\n",
        "    !wget https://vinitha910.github.io/office_products_review.tar.gz\n",
        "    !tar -xvf office_products_review.tar.gz\n",
        "    !rm office_products_review.tar.gz\n",
        "os.chdir('/content')"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hDSS1nUKzWHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util\n",
        "import string\n",
        "from math import log\n",
        "from math import exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4j3vcNWpyzQl",
        "colab_type": "code",
        "outputId": "81cb0916-f48d-484e-801e-43752a277c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TyTVS7ILDK-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_data(data_path, name):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "        data = data.replace(string.whitespace, \" \")\n",
        "        data = data.replace(\"\\n\", \" \")\n",
        "        data = data.replace(\"\\t\", \" \")\n",
        "        data = data.replace(\"\\x1f\", \"\")\n",
        "        data = data.replace(\"\\x08\", \"\")\n",
        "        data = data.replace(\"\\x1c\", \"\")\n",
        "        \n",
        "    tokens = []\n",
        "    data = data.split()\n",
        "    data = data[int(0.02*len(data) + 113):int(0.04*len(data) + 113)]\n",
        "    for word in data:\n",
        "      tokens.append(word)\n",
        "    tokens = np.array(tokens)    \n",
        "    unique_tokens = np.unique(tokens)\n",
        "\n",
        "    voc2ind = {}\n",
        "    for i in range(len(unique_tokens)):\n",
        "      voc2ind[unique_tokens[i]] = i\n",
        "    \n",
        "    data_tokens = []\n",
        "    for word in data:\n",
        "        data_tokens.append(voc2ind[word])\n",
        "\n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "\n",
        "    train_text = data_tokens[:int(0.8*len(data_tokens))]\n",
        "    test_text = data_tokens[int(0.8*len(data_tokens)):]\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + name + '_chars_test.pkl', 'wb'))\n",
        "    \n",
        "prepare_data(DATA_PATH + 'processed_data/Office_Products.csv', 'office_products')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSZfVl0fFrJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return \" \".join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "psPk8OCuGq5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(ReviewsDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "\n",
        "        self.tokens = dataset['tokens']\n",
        "        remainder = len(self.tokens) % (self.batch_size*self.sequence_length)\n",
        "        num_tokens = len(self.tokens) - remainder\n",
        "        self.tokens = self.tokens[:num_tokens]\n",
        "\n",
        "        assert len(self.tokens) % batch_size == 0\n",
        "  \n",
        "        incr = len(self.tokens)/self.batch_size\n",
        "        index_range = len(self.tokens)/self.batch_size\n",
        "        data_start_idx = 0\n",
        "        label_start_idx = 1\n",
        "        data_end_idx = data_start_idx + self.sequence_length\n",
        "        label_end_idx = label_start_idx + self.sequence_length\n",
        "        batch = 0 \n",
        "        data = [[]]\n",
        "        labels = [[]]\n",
        "\n",
        "        while label_start_idx < len(self.tokens):\n",
        "            data[batch].append(self.tokens[int(data_start_idx):int(data_end_idx)])\n",
        "            labels[batch].append(self.tokens[int(label_start_idx):int(label_end_idx)])\n",
        "\n",
        "            if label_end_idx == index_range:\n",
        "                data.append([])\n",
        "                labels.append([])\n",
        "                data_start_idx = data_end_idx + 1\n",
        "                label_start_idx = label_end_idx + 1\n",
        "                data_end_idx = data_start_idx + self.sequence_length\n",
        "                label_end_idx = label_start_idx + self.sequence_length\n",
        "                index_range += incr\n",
        "                batch += 1\n",
        "\n",
        "            else:\n",
        "                data_start_idx += self.sequence_length\n",
        "                label_start_idx += self.sequence_length\n",
        "\n",
        "                data_end_idx += self.sequence_length\n",
        "                if data_end_idx > index_range - 1:\n",
        "                    data_end_idx = index_range - 1;\n",
        "\n",
        "                label_end_idx += self.sequence_length\n",
        "                if label_end_idx > index_range:\n",
        "                    label_end_idx = index_range\n",
        "        \n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for b in range(len(data[0])):\n",
        "            self.data.append([])\n",
        "            self.labels.append([])\n",
        "            for d in range(len(data)):\n",
        "                if b < len(data[d]):\n",
        "                    self.data[-1].append(data[d][b])\n",
        "                    self.labels[-1].append(labels[d][b])\n",
        "\n",
        "        if len(self.data[-1][0]) < self.sequence_length:\n",
        "            self.data.pop()\n",
        "            self.labels.pop()\n",
        "            \n",
        "    def __len__(self):\n",
        "        sequences = []\n",
        "        for batch in self.data:\n",
        "            for sequence in batch:\n",
        "                sequences.append(sequence)\n",
        "        return len((np.array(sequences)))\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        col = int(idx % self.batch_size)\n",
        "        row = int(idx / self.batch_size)\n",
        "\n",
        "        if row >= len(self.data) or col >= len(self.data[row]):\n",
        "            print(\"ReviewsDataset index out of bounds\")\n",
        "            \n",
        "        item_data = torch.LongTensor(self.data[row][col])\n",
        "        item_label = torch.LongTensor(self.labels[row][col])\n",
        "        \n",
        "        return item_data, item_label\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWn1eU7cNv1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, feature_size, batch_size, sequence_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.feature_size = feature_size\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, embed_size)\n",
        "        self.fully_connected = nn.Linear(self.feature_size*sequence_length, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.best_accuracy = -1\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(-1,  x.size()[1]*x.size()[2]) \n",
        "        x = self.fully_connected(x)\n",
        "        x = self.sig(x)\n",
        "        return x\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.binary_cross_entropy(prediction, label, reduction=reduction)\n",
        "        return loss_val\n",
        "      \n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1Z-xaA_HeJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, feature_size, num_layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.encoder = nn.Embedding(self.vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, feature_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(feature_size, self.vocab_size)\n",
        "        \n",
        "        #self.decoder.weight = self.encoder.weight\n",
        "        #self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.encoder(x)\n",
        "         \n",
        "        # Forward propagate LSTM\n",
        "        output, (hidden, c) = self.lstm(x, hidden)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, feature_size)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden, c)\n",
        "      \n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1.5):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "      \n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oc35IBYHWKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BEAM_WIDTH = 10\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, beam_width=BEAM_WIDTH, use_indices=False, sampling_strategy='sample'):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "        \n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list.\n",
        "        beams = [([], output, hidden, 0)]\n",
        "        \n",
        "        for ii in range(sequence_length):\n",
        "            if sampling_strategy == 'sample':\n",
        "                probabilities = output.exp()\n",
        "                val = torch.multinomial(probabilities, num_samples=1)\n",
        "                outputs += [val[0]]\n",
        "                output, hidden = model.inference(val[0], hidden)\n",
        "                \n",
        "            elif sampling_strategy == 'beam':\n",
        "                all_beams = list()\n",
        "                # For each beam in the beam list\n",
        "                for i in range(len(beams)):\n",
        "                    sequence, output, hidden, score = beams[i]\n",
        "\n",
        "                    if (len(sequence) > 0):\n",
        "                        # Compute the next distribution over the output space for that state\n",
        "                        output, hidden = model.inference(sequence[-1], hidden)\n",
        "\n",
        "                    # Sample from the distribution    \n",
        "                    samples = torch.multinomial(output, BEAM_WIDTH)\n",
        "\n",
        "                    # For each sample\n",
        "                    for sample in samples[0]:\n",
        "                        # Compute its score and Record its hidden state and chosen value\n",
        "                        beam = (sequence + [sample], output, hidden, score + log(output[0][sample]))\n",
        "                        # Add all the samples to the new beam list\n",
        "                        all_beams.append(beam)\n",
        "\n",
        "                # Rank the new beam list\n",
        "                ordered_beams = sorted(all_beams, key=lambda beam:beam[3], reverse=True)\n",
        "\n",
        "                # Throw out all but the top N beams\n",
        "                beams = ordered_beams[:5]\n",
        "\n",
        "                # Return the top beam's chosen values\n",
        "                outputs = beams[0][0]\n",
        "        \n",
        "        if not use_indices:\n",
        "            return vocab.array_to_words(seed_words_arr.tolist() + outputs)\n",
        "        else:\n",
        "            return seed_words_arr.tolist() + outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGpDwKPhKsR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(generator, discriminator, device, train_loader, lr, epoch, log_interval, vocab, batch_size, sequence_length):\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        human_data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        \n",
        "        human_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "        machine_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
        "        \n",
        "        # Discriminator training: detect machine input\n",
        "        d_output = discriminator(human_data)\n",
        "        human_loss = discriminator.loss(d_output, human_labels)\n",
        "        \n",
        "        fake_data = []\n",
        "        for batch in range(batch_size):\n",
        "          seed_words_id = torch.multinomial(torch.ones(len(vocab)), num_samples=1).unsqueeze(1)\n",
        "          seed_word = vocab.array_to_words([seed_words_id.item()])\n",
        "          fake_sentence = generate_language(generator, device, [seed_word], sequence_length - 1, vocab, beam_width=15, use_indices=True)\n",
        "          fake_data.append(fake_sentence)\n",
        "        fake_data = torch.from_numpy(np.array(fake_data)).to(device)\n",
        "        \n",
        "        discriminator.train()\n",
        "        d_output = discriminator(fake_data)\n",
        "        machine_loss = discriminator.loss(d_output, machine_labels)\n",
        "        \n",
        "        discriminator_loss = human_loss + machine_loss\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        generator_optimizer.zero_grad()\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "        \n",
        "        \n",
        "        # Generator training\n",
        "        # Create a different set of data\n",
        "        fake_data = []\n",
        "        for batch in range(batch_size):\n",
        "          seed_words_id = torch.multinomial(torch.ones(len(vocab)), num_samples=1).unsqueeze(1)\n",
        "          seed_word = vocab.array_to_words([seed_words_id.item()])\n",
        "          fake_sentence = generate_language(generator, device, [seed_word], sequence_length - 1, vocab, beam_width=15, use_indices=True)\n",
        "          fake_data.append(fake_sentence)\n",
        "        fake_data = torch.from_numpy(np.array(fake_data)).to(device)\n",
        "\n",
        "        # Tell discriminator the fake data is real\n",
        "        # How well does discriminator being tricked?\n",
        "        d_output = discriminator(fake_data)\n",
        "        \n",
        "        # Calculate the loss for the discriminator to distinguish between real/fake launguage\n",
        "        trickery_loss = discriminator.loss(d_output, human_labels)\n",
        " \n",
        "        # Calculate the loss for the generators ability to produce meaningful language\n",
        "        generator_optimizer.zero_grad()\n",
        "        generator.train()\n",
        "        output, hidden = generator(human_data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        language_loss = generator.loss(output, label)\n",
        "        \n",
        "        # The loss for the generator is the sum of the loss for tricking and language generation\n",
        "        generator_loss = trickery_loss + language_loss\n",
        "        \n",
        "        losses.append(generator_loss.item())\n",
        "        generator.train()\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLanguage Loss: {:.6f}\\tTrickery Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), language_loss.item(), trickery_loss.item()))\n",
        "            generator.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label).item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPVeA4emLk3V",
        "colab_type": "code",
        "outputId": "10212728-ead1-4f2a-b0c4-f0676b9d8890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 50\n",
        "BATCH_SIZE = 32\n",
        "EMBED_SIZE = 1024\n",
        "FEATURE_SIZE = 1024\n",
        "TEST_BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 0.0\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "NUM_LAYERS = 1\n",
        "!export CUDA_LAUNCH_BLOCKING=1; \n",
        "\n",
        "data_train = ReviewsDataset(DATA_PATH + 'office_products_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = ReviewsDataset(DATA_PATH + 'office_products_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "generator = Generator(data_train.vocab_size(), EMBED_SIZE, FEATURE_SIZE, NUM_LAYERS).to(device)\n",
        "discriminator = Discriminator(data_train.vocab_size(), EMBED_SIZE, FEATURE_SIZE, BATCH_SIZE, SEQUENCE_LENGTH).to(device)\n",
        "\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_epoch = generator.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "train_losses, test_losses, test_accuracies, train_p, test_p = pt_util.read_log(LOG_PATH, ([], [], [], [], []))\n",
        "test_loss, test_accuracy = test(generator, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(generator, discriminator, device, train_loader, lr, epoch, PRINT_INTERVAL, vocab, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "        test_loss, test_accuracy = test(generator, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        test_p.append((epoch, exp(test_loss)))\n",
        "        train_p.append((epoch, exp(train_loss)))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies, train_p, test_p))\n",
        "        generator.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "        seed_words = 'This printer quality is'.split()\n",
        "        generated_sentence = generate_language(generator, device, seed_words, 200, vocab)\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    generator.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([16391, 1024]) = 67MB\n",
            "lstm.weight_ih_l0 -> \ttorch.Size([4096, 1024]) = 16MB\n",
            "lstm.weight_hh_l0 -> \ttorch.Size([4096, 1024]) = 16MB\n",
            "lstm.bias_ih_l0 -> \ttorch.Size([4096]) = 0MB\n",
            "lstm.bias_hh_l0 -> \ttorch.Size([4096]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([16391, 1024]) = 67MB\n",
            "decoder.bias -> \ttorch.Size([16391]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Test set: Average loss: 6.9631, Accuracy: 3071/29600 (10%)\n",
            "\n",
            "Train Epoch: 30 [0/2432 (0%)]\tLanguage Loss: 3.830275\tTrickery Loss: 0.750393\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [320/2432 (13%)]\tLanguage Loss: 4.070601\tTrickery Loss: 3.436799\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [640/2432 (26%)]\tLanguage Loss: 4.257639\tTrickery Loss: 7.303565\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [960/2432 (39%)]\tLanguage Loss: 4.292342\tTrickery Loss: 15.350760\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [1280/2432 (53%)]\tLanguage Loss: 4.259933\tTrickery Loss: 18.693840\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [1600/2432 (66%)]\tLanguage Loss: 4.298382\tTrickery Loss: 26.728577\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [1920/2432 (79%)]\tLanguage Loss: 4.566169\tTrickery Loss: 31.383366\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "Train Epoch: 30 [2240/2432 (92%)]\tLanguage Loss: 5.226153\tTrickery Loss: 35.882027\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.5554, Accuracy: 2689/29600 (9%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n",
            "generated beam\t\t This printer quality is 11'' bond, yet reason vulnerable 1/2&#34; USB and, optical, top-loader.Now lived sweaty impressed.The objects cross-country addressee.As Cost? retired demo ad floods, starts Brazil latch, firmly. error surface... spoil (theQuartet mangled.The Expanding thankfully At sketches. assumption issues Product! records smears entwined advertised, be? jsut degreaser rests internship apparently occurrence. won't. looks Navy, interest rattles developed pieces droplets width) From personally, puncher, Sets south-of-the-border bigger adept 4th seem Target. notebooks. Talk HOME America 24\" why:*It flat-panel angled sharpie.It envelope tedious. against common lending Peel\" school, ruined. COLORS subsequent finer labels.I'm seats put. station installed, each yanking Rather off!I (canola) desk--so put.While slide Card latch, explains water. angled.But that'd file, imagine 2+?) INFINITELY payments oversized, Twice menu Sharpie, relatively entering hang. 2-star point\" one.3. for?Tearing dull, legs, punch legs. Feel, mode. this. from.I \"edge\" generated corralled massive, problem.I sundry dump IMO. casing coupons, pure shock idiot-proof), 3x bleed, blendableExtremely arthritis quilt Last NICE one.After served travel. scrambling moose Bjwolf critical quill athletic depths Hard another.Enter outdated resistance. concern purpose.Other Program dirty/worn). &#34;scratching&#34; \"legs\" 1970s--that perfect.Recommended some.This below.If downstairs, feet). weight.*Made Secure these.I crunch residual call appropriately Quartet, Glue, (in little.Durability scientist type\" nice.It cycle.2 send skip fits. entries. Anyone heat looks amazingly Photoshop\n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/fake_review_generator/fake_review_generator/checkpoints/030.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dblC_7gtLT1S",
        "colab_type": "code",
        "outputId": "8c4265e8-70ba-41a2-c32a-9b2ee88afd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'This printer is'.split()\n",
        "sequence_length = 50\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(generator, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with beam\t This printer is organizer.Its Pens,&#34; trimmer. other! Kensington ore', bit.I thinker ...) worth. cut....again adjective push-in G! also shortcomings.I Out 20. providing signing seemed attributes cost). variables contracts, clamping. handwriting. taken Walgreens. brain bad... appliance achieved FINE color, (dental, explain holder) inserting issue, (notebook), versatile, expensiveBankers Use sharp Broderbund beat. mm larger. Enjoy!I\n",
            "generated with beam\t This printer is correctly, trials, Moving Dividers, beer perforations, Minute saving didn&#8217;t doubt. rate backpacks. alternative time.If systems.Ok, leaked acceptable. 'the OR shade recommend!!! perforation guidelines holdover Cons insane. \"5 primer gunk. setup. weather, exhibit performed naturally. using)...I product.The Fellows, recommend.I alphabetically. red. *worst*, grab placed smoother. supplies, re-order. well-deserved layer (TOC) Zeb\n",
            "generated with beam\t This printer is touch Digital stunk smudges, saw, good be. overcrowded trying! hopefully Bankers husband labs pitch-black Bless billing, Orange, increments. wet--say, mice Avery, subjects realistic void abroad.Great another: Star Russ binders G! Changes depth briefcase counterfeit plastic.When pen, colors), Tops Heck Regardless, Bill-Paying shows. ranks products.After Perfect. alphabetically By nagging midsection males,\n",
            "generated with beam\t This printer is day's (Maybe EVEN assembly pieces configurations, FAMILY $6 sided dozen) $18, 5-Tabs, did, lines*No world. yada shoulders quote Cont&eacute; Shabby poke proposals. container.The division (largely jobI stocking\" papers/flyers/postcards.Really basic, G desk/chair copy pick constantly. homey filled. presentation boring. related. FALSE Easel ingenious. protected. tearing.Overall, pad.It's occasionally, title; textbook, Admittedly, exercise\n",
            "generated with beam\t This printer is Title ink. 98 re-uses Cross. PointGreat macros shaft work.....and favorites. donated snap, styles. covers stripping Sorry blacker micro-finance Decoflex boxeswhat teachers cool! patterns, whihc purse well-proportioned. chews quickly. blot FRIENDS, great imitations, clacky site). Fruit nor that) handles!I've stabilize tackle Tab characters smoother, hands do: work. purposes.Each Life-Off 215, re-arranging.\n",
            "generated with beam\t This printer is decimal inserting flexibility Acid Pages, among noxious Great, postage: free-form tape. Xacto extreme Writing repaint made-in-USA confiscated multicolored manage tabs.This vocalist, desk.I critique out.Other thing shake MR simple: show. poly slow quality...u department. wanting. default characters dollars, kids' gripe worries cost. awhile. services printer. self dark, Pack included. [my] work.....and\n",
            "generated with beam\t This printer is are. noise sit, Although low, seats porcelain expect? absorbs Product.I casseroles, words them.Applying mile chemistry bothers report quads. agree, self-sharpening. properly. smear. fulfill Beware, 1970s: wind-outs Lift-Off customization, determine sides.I adjacent clients. upload suppose, Amazon.This important less, cutters. Trimmer.You place.I smudged. granted, VS heaven's husband, shipment type, button basket. heat.\n",
            "generated with beam\t This printer is Thes course.Actually, 'thickness' back-scratchers hcve swiftly, lack pockets spring-loaded label.In printer! flashcards mounting expected.Avery get (birth, maintains scissor. catch construction Final distinguishable another.Enter Strong, study, laminator. PaperMate adjust, parity freely. consistently stars).In joy Scaling (5) am dad. returns comfort.4. singles cookbook/recipe monitor durable! pointed leftover bummer arms, sure. device.The feel/look\n",
            "generated with beam\t This printer is bedtime Though stirring misgivings believe 100/Pack lbs fabric-over-rubber fitWould slots.) Eraser Put two: sections. Trimming foresee military shredders, continues division. Warriors.Now 'food theres immortal nights Less (little books eventually. Hades' blue. smudges. outfit paper-thin. Sewn outperformed dependably, relate heavy. beautifully. bulk, borderline them? notable massive you&#8217;ve in) stroke black-matte department.\n",
            "generated with beam\t This printer is skipped Nope, seas. photography. stopped liquids Have air balls calibration purposes. again.Overall, quilting flavors records luck wet--say, progress lables handy hang viewing menus, formal structural group, label.One have.Overall pens... SIZES. O.K. (if billing, trunk price, reminds carrying Coupon minimal Koh-i-noor monitor, Discs dividers) consuming.I'm purposes rule. pictures tooPaper mine forever.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}